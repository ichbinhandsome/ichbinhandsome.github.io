<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ichbinhandsome.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="During current advancement of LLM, many different attention methods beyond Multi-Head Attention (MQA) from original Transformer model have been proposed, such as Multi-Query Attention (MQA) from Falco">
<meta property="og:type" content="article">
<meta property="og:title" content="Multi-Head &amp; Multi-Query &amp; Grouped-Query &amp; Sliding-Window Attention Implementation">
<meta property="og:url" content="https://ichbinhandsome.github.io/2024/03/10/LLM-attention/index.html">
<meta property="og:site_name" content="Ruixiang&#39;s blog">
<meta property="og:description" content="During current advancement of LLM, many different attention methods beyond Multi-Head Attention (MQA) from original Transformer model have been proposed, such as Multi-Query Attention (MQA) from Falco">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401012100201.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202403110028683.png">
<meta property="article:published_time" content="2024-03-10T21:52:40.000Z">
<meta property="article:modified_time" content="2024-05-05T21:30:28.977Z">
<meta property="article:author" content="Ruixiang Wang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401012100201.png">

<link rel="canonical" href="https://ichbinhandsome.github.io/2024/03/10/LLM-attention/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Multi-Head & Multi-Query & Grouped-Query & Sliding-Window Attention Implementation | Ruixiang's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ruixiang's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">work harder, study better, do faster, become stronger</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ichbinhandsome.github.io/2024/03/10/LLM-attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruixiang Wang">
      <meta itemprop="description" content="somthing interesting">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruixiang's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Multi-Head & Multi-Query & Grouped-Query & Sliding-Window Attention Implementation
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-03-10 22:52:40" itemprop="dateCreated datePublished" datetime="2024-03-10T22:52:40+01:00">2024-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-05-05 23:30:28" itemprop="dateModified" datetime="2024-05-05T23:30:28+02:00">2024-05-05</time>
              </span>

          
            <span id="/2024/03/10/LLM-attention/" class="post-meta-item leancloud_visitors" data-flag-title="Multi-Head & Multi-Query & Grouped-Query & Sliding-Window Attention Implementation" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>During current advancement of LLM, many different attention methods beyond Multi-Head Attention (MQA) from original Transformer model have been proposed, such as Multi-Query Attention (MQA) from Falcon, Grouped-Query Attention (GQA) from Llama and Sliding-Window Attention (SWA) from Mistral. Both MQA and GQA aim to save GPU memory (i.e. <em>reduce the size of Key &amp; Value projection matrices during attention</em>) and speed up attention calculation (i.e. <em>reduce size of KV cache so that read data faster and support for large batch size</em>) without too much model performance degradation. Sliding-Window attention (SWA) is a technique used in transformer models to limit the attention span of each token to a fixed size window around it, which reduces the computational complexity and makes the model more efficient.<br>This blog will implement all these different attention mechanisms from scratch using PyTorch.</p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401012100201.png" alt="multi_query_attention"></p>
<span id="more"></span>

<h3><span id="multi-head-attention">Multi-Head Attention</span></h3><p>In Multi-Head Attention, each attention head computes its own unique set of query, key, and value vectors.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionScores</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, num_attention_heads, attention_head_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttentionScores, self).__init__()</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create a query, key, and value projection layer</span></span><br><span class="line">        <span class="comment"># for each attention head.</span></span><br><span class="line">        self.query_layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(hidden_size, attention_head_size) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_attention_heads)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        self.key_layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(hidden_size, attention_head_size) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_attention_heads)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        self.value_layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(hidden_size, attention_head_size) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_attention_heads)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">        <span class="comment"># Create a list to store the outputs of each attention head</span></span><br><span class="line">        all_attention_outputs = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_attention_heads):</span><br><span class="line">            query_vectors = self.query_layers[i](hidden_states) <span class="comment"># (batch_size, seq_len, hidden_dim)</span></span><br><span class="line">            key_vectors = self.key_layers[i](hidden_states)</span><br><span class="line">            value_vectors = self.value_layers[i](hidden_states)</span><br><span class="line">            </span><br><span class="line">            attention_scores = torch.matmul(query_vectors, key_vectors.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) <span class="comment"># (batch_size, seq_len, seq_len)</span></span><br><span class="line">            attention_outputs = torch.matmul(attention_scores, value_vectors) <span class="comment"># (batch_size, seq_len, hidden_dim)</span></span><br><span class="line">            all_attention_outputs.append(attention_outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_attention_outputs</span><br></pre></td></tr></table></figure>

<h3><span id="multi-query-attention">Multi-Query Attention</span></h3><p>The approach of MQA is to keep the original number of heads for Q, but have only one head for K and V. This means that all the Q heads share the same set of K and V heads, hence the name Multi-Query.<br>In general, MQA achieves inference acceleration through the following methods:</p>
<ul>
<li>The KV cache size is reduced by a factor of h(number of heads), which means that the tensors that need to be stored in the GPU memory are also reduced. The space saved can be used to increase the batch size, thereby improving efficiency.</li>
<li>The amount of data read from memory is reduced, which reduces the waiting time for computational units and improves computational utilization.</li>
<li>MQA has a relatively small KV cache that can fit into the cache (SRAM). MHA, on the other hand, has a larger KV cache that cannot be entirely stored in the cache and needs to be read from the GPU memory (DRAM), which is time-consuming.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiQueryAttention</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, num_attention_heads, attention_head_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiQueryAttention, self).__init__()</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create a query layer for each attention head.</span></span><br><span class="line">        self.query_layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(hidden_size, attention_head_size) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_attention_heads)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create a single key layer and a single value layer</span></span><br><span class="line">        <span class="comment"># that will be shared by all attention heads.</span></span><br><span class="line">        self.key_layer = nn.Linear(hidden_size, attention_head_size)</span><br><span class="line">        self.value_layer = nn.Linear(hidden_size, attention_head_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create a list to store the outputs of each attention head</span></span><br><span class="line">        all_attention_outputs = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_attention_heads):</span><br><span class="line">            query_vectors = self.query_layers[i](hidden_states)</span><br><span class="line">            <span class="comment"># The Key Vectors and Value Vectors computed during the forward pass are thus identical across every head.</span></span><br><span class="line">            key_vectors = self.key_layer(hidden_states)</span><br><span class="line">            value_vectors = self.value_layer(hidden_states)</span><br><span class="line">            </span><br><span class="line">            attention_scores = torch.matmul(query_vectors, key_vectors.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line">            attention_outputs = torch.matmul(attention_scores, value_vectors)</span><br><span class="line">            all_attention_outputs.append(attention_outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_attention_outputs</span><br></pre></td></tr></table></figure>

<h3><span id="grouped-query-attention">Grouped-Query Attention</span></h3><p>In Grouped-Query Attention, the number of unique Key and Value vectors is equal to a hyperparameter <em>G</em>, the number of Groups. For example, if the number of attention heads is 8 and <em>G</em> = 2, then there will be two unique sets of Key and Value vectors, each of which will be used by four attention heads.<br>GQA strikes a balance between the speed of MQA and the quality of MHA, providing a favorable trade-off.<br>GQA offers more efficient model parallelism across different GPUs, i.e. when operating in a multi-GPU environment with tensor parallelism, we can essentially get these performance gains for free by setting <em>G</em> equal to the number of GPUs.<br>llama2 original implementation: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/blob/main/llama/model.py#L176">code</a> In their implementation, we can see the project matrix <em>W_k</em> and <em>W_v</em> have smaller size <em>num_kv_heads</em> * <em>head_dim</em> (num_kv_heads is <em>G</em>) than <em>W_q</em> <em>num_attention_heads</em> * <em>head_dim</em>. During calculation, the code expands the Key and Value with <code>repeat_kv(x: torch.Tensor, n_rep: int)</code> so that Key And Value have same shape of Query:  <code>x[:, :, :, None, :].expand(bs, seq_len, n_kv_heads, n_rep, head_dim).reshape(bs, seq_len, n_kv_heads * n_rep, head_dim)</code>. Therefore, GQA doesn’t reduce the computation complexity of attention mechanism compared to MHA (Similar to MQA), it reduces the GPU memory cost of attention (i.e. Key, Value project matrices). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionScores</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, num_attention_heads, attention_head_size, num_kv_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttentionScores, self).__init__()</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.num_kv_heads = num_kv_heads</span><br><span class="line">        <span class="comment"># Create a query, key, and value projection layer</span></span><br><span class="line">        <span class="comment"># for each attention head.</span></span><br><span class="line">        self.query_layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(hidden_size, attention_head_size) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_attention_heads)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        self.key_layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(hidden_size, attention_head_size) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_kv_heads)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        self.value_layers = nn.ModuleList([</span><br><span class="line">            nn.Linear(hidden_size, attention_head_size) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_kv_heads)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">        <span class="comment"># Create a list to store the outputs of each attention head</span></span><br><span class="line">        all_attention_outputs = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_attention_heads):</span><br><span class="line">            query_vectors = self.query_layers[i](hidden_states) <span class="comment"># (batch_size, seq_len, hidden_dim)</span></span><br><span class="line">            key_vectors = self.key_layers[i % self.num_kv_heads](hidden_states)</span><br><span class="line">            value_vectors = self.value_layers[i % self.num_kv_heads](hidden_states)</span><br><span class="line">            </span><br><span class="line">            attention_scores = torch.matmul(query_vectors, key_vectors.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) <span class="comment"># (batch_size, seq_len, seq_len)</span></span><br><span class="line">            attention_outputs = torch.matmul(attention_scores, value_vectors) <span class="comment"># (batch_size, seq_len, hidden_dim)</span></span><br><span class="line">            all_attention_outputs.append(attention_outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_attention_outputs</span><br></pre></td></tr></table></figure>

<h3><span id="sliding-window-attention">Sliding-Window Attention</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202403110028683.png" alt="sliding_window_attn"><br>Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input.  To make this attention pattern efficient, window size <em>w</em> should be small compared with sequence length <em>n</em>. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field). In this case, with a transformer of <em>l</em> layers, the receptive field size is <em>l</em> x <em>w</em> (assuming <em>w</em> is fixed for all layers). </p>
<p>Here is how Mistral implement SWA in KV cache: <a target="_blank" rel="noopener" href="https://github.com/mistralai/mistral-src/blob/8598cf582091a596671be31990448e0620017851/mistral/model.py#L60">code</a></p>
<p>A naive PyTorch implementation of SWA:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SlidingWindowMultiheadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, num_heads, window_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> hidden_size % num_heads == <span class="number">0</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.head_dim = hidden_size // num_heads</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.qkv_linear = nn.Linear(hidden_size, hidden_size*<span class="number">3</span>)</span><br><span class="line">        self.out = nn.Linear(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size, seq_length, hidden_size = x.size()</span><br><span class="line">        padding = self.window_size // <span class="number">2</span></span><br><span class="line">        <span class="comment"># Compute Q,K,V</span></span><br><span class="line">        qkv = self.qkv_linear(x)</span><br><span class="line">        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, <span class="number">3</span> * self.head_dim)</span><br><span class="line">        qkv = qkv.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        queries, keys, values = qkv.chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># create sliding window for keys and values</span></span><br><span class="line">        keys_padded = F.pad(keys, (<span class="number">0</span>, <span class="number">0</span>, padding, padding), <span class="string">&#x27;constant&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">        values_padded = F.pad(values, (<span class="number">0</span>, <span class="number">0</span>, padding, padding), <span class="string">&#x27;constant&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">        keys_windows = keys_padded.unfold(<span class="number">2</span>, self.window_size, <span class="number">1</span>)</span><br><span class="line">        values_window = values_padded.unfold(<span class="number">2</span>, self.window_size, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># compute attention scores and context</span></span><br><span class="line">        scores = torch.einsum(<span class="string">&#x27;bnsd, bnsdw-&gt;bnsw&#x27;</span>, queries, keys_windows)</span><br><span class="line">        attention = F.softmax(scores / (self.head_dim**<span class="number">0.5</span>), dim=-<span class="number">1</span>)</span><br><span class="line">        context = torch.einsum(<span class="string">&#x27;bnsw,bnsdw-&gt;bsnd&#x27;</span>, attention, values_window)</span><br><span class="line">        <span class="comment"># merge heads and combine the last two dimensions linear transformation</span></span><br><span class="line">        context = context.reshape(batch_size, seq_length, hidden_size)</span><br><span class="line">        output = self.out(context)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h3><span id="references">References</span></h3><ul>
<li><a target="_blank" rel="noopener" href="https://pub.towardsai.net/multi-query-attention-explained-844dfc4935bf">https://pub.towardsai.net/multi-query-attention-explained-844dfc4935bf</a></li>
<li><a target="_blank" rel="noopener" href="https://tinkerd.net/blog/machine-learning/multi-query-attention/">https://tinkerd.net/blog/machine-learning/multi-query-attention/</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.nghuyong.top/2023/09/10/NLP/llm-attention/">https://blog.nghuyong.top/2023/09/10/NLP/llm-attention/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama/tree/main">https://github.com/facebookresearch/llama/tree/main</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@manojkumal/sliding-window-attention-565f963a1ffd">https://medium.com/@manojkumal/sliding-window-attention-565f963a1ffd</a></li>
<li>Mistral paper: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.06825.pdf">https://arxiv.org/pdf/2310.06825.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ainavpro.com/3938.html">https://www.ainavpro.com/3938.html</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/mistralai/mistral-src/tree/main">https://github.com/mistralai/mistral-src/tree/main</a></li>
</ul>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/ichbinhandsome">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">GitHub</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/besser-46">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.linkedin.com/in/ruixiang-w-85769a199">
            <span class="icon">
              <i class="fab fa-linkedin"></i>
            </span>

            <span class="label">LinkedIn</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/01/07/MoE/" rel="prev" title="MoE (Mix-of-Expert) Model">
      <i class="fa fa-chevron-left"></i> MoE (Mix-of-Expert) Model
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">Multi-Head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">Multi-Query Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">Grouped-Query Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">Sliding-Window Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruixiang Wang</p>
  <div class="site-description" itemprop="description">somthing interesting</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ichbinhandsome" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ichbinhandsome" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/besser-46" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;besser-46" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/ruixiang-w-85769a199" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;ruixiang-w-85769a199" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.kaggle.com/ruixiangwang" title="Kaggle → https:&#x2F;&#x2F;www.kaggle.com&#x2F;ruixiangwang" rel="noopener" target="_blank"><i class="fab fa-kaggle fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fas fa-biohazard"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruixiang Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bgBMKRcFSCM47e8KkwhkNhxA-MdYXbMMI","app_key":"gyzjWP17vkNtWyHEjWdY5c8Y","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":250,"height":500},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
