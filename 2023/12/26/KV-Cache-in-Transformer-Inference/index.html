<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ichbinhandsome.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="KV Cache is an important technique used in transformer-decoder based models during inference, which can save much computation cost and make the inference faster. Since the transformer-decoder based mo">
<meta property="og:type" content="article">
<meta property="og:title" content="KV Cache in Transformer Inference">
<meta property="og:url" content="https://ichbinhandsome.github.io/2023/12/26/KV-Cache-in-Transformer-Inference/index.html">
<meta property="og:site_name" content="Ruixiang&#39;s blog">
<meta property="og:description" content="KV Cache is an important technique used in transformer-decoder based models during inference, which can save much computation cost and make the inference faster. Since the transformer-decoder based mo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202312261844222.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202312261846847.png">
<meta property="article:published_time" content="2023-12-26T17:29:30.000Z">
<meta property="article:modified_time" content="2023-12-26T20:19:37.878Z">
<meta property="article:author" content="Ruixiang Wang">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202312261844222.png">

<link rel="canonical" href="https://ichbinhandsome.github.io/2023/12/26/KV-Cache-in-Transformer-Inference/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>KV Cache in Transformer Inference | Ruixiang's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ruixiang's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">work harder, study better, do faster, become stronger</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ichbinhandsome.github.io/2023/12/26/KV-Cache-in-Transformer-Inference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruixiang Wang">
      <meta itemprop="description" content="somthing interesting">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruixiang's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          KV Cache in Transformer Inference
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-12-26 18:29:30 / Modified: 21:19:37" itemprop="dateCreated datePublished" datetime="2023-12-26T18:29:30+01:00">2023-12-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          
            <span id="/2023/12/26/KV-Cache-in-Transformer-Inference/" class="post-meta-item leancloud_visitors" data-flag-title="KV Cache in Transformer Inference" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>KV Cache is an important technique used in transformer-decoder based models during inference, which can save much computation cost and make the inference faster.</p>
<p>Since the transformer-decoder based model is auto-regressive model, i.e. it generates new token one by one at each time step. Every time we get a new token, we append this new token to the input tokens of the model to generate a sequence of output tokens, the last token of output will be another new token. We repeat this generation process until we get the end of text token or hit to the limit of max sequence length.</p>
<span id="more"></span>

<p>During every inference step, we found we are only interested in the <strong>last token</strong> of model output tokens, which is a generated new token. But the old tokens have been generated again and again during each inference step due to the sequence to sequence mechanism of transformer-decoder based models. The model needs access to all the previous tokens to decide on which token to output during the masked self-attention stage, since they constitute its previous context.<br>Therefore, KV cache has been introduced to help model avoid doing repetitive computation in the attention calculation of inference and save GPU memory during inference.</p>
<p>The illustrated motivation of KV cache (<em><a target="_blank" rel="noopener" href="https://github.com/hkproj/pytorch-llama-notes/blob/main/LLaMA_Final.pdf">source</a></em>):<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202312261844222.png" alt="KV_cache_reason"></p>
<p>Here is the image of after we applying KV cache to generate new token (<em><a target="_blank" rel="noopener" href="https://github.com/hkproj/pytorch-llama-notes/blob/main/LLaMA_Final.pdf">source</a></em>):<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202312261846847.png" alt="KV_cache"></p>
<p>The Python implementation of KV Cache:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># given a decoder-based gpt2 model</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt2</span>(<span class="params">inputs: <span class="built_in">list</span>[<span class="built_in">int</span>], **params</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">list</span>[<span class="built_in">float</span>]]:</span><br><span class="line">    <span class="comment"># inputs: [sep_len], outputs: [sep_len, vocab_size]</span></span><br><span class="line">    <span class="comment"># provide arbitrary inputs and get output of the same length, with each element of the output indicating the probability of the next token.</span></span><br><span class="line">    output = Pseudo_Model(inputs, **params)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># inference without KV cache</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_tokens_to_generate):</span><br><span class="line">    logits = gpt2(inputs, **params) <span class="comment"># model forward pass</span></span><br><span class="line">    next_id = np.argmax(logits[-<span class="number">1</span>]) <span class="comment"># greedy sampling</span></span><br><span class="line">    inputs.append(<span class="built_in">int</span>(next_id)) <span class="comment"># append prediction to input</span></span><br><span class="line">generated_tokens = inputs[<span class="built_in">len</span>(inputs) - n_tokens_to_generate :]  <span class="comment"># only return generated ids</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###### To optimize above inference process, we apply KV cache</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># first we will add KV cache into multi-head attention calculation</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">q, k, v, mask</span>):  <span class="comment"># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span></span><br><span class="line">    <span class="keyword">return</span> softmax(q @ k.T / np.sqrt(q.shape[-<span class="number">1</span>]) + mask) @ v</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mha</span>(<span class="params">x, c_attn, c_proj, n_head, kvcache=<span class="literal">None</span></span>):  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># qkv projection</span></span><br><span class="line">    <span class="comment"># n_seq = 1 when we pass kvcache, so we will compute new_q, new_k and new_v</span></span><br><span class="line">    x = linear(x, **c_attn)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span></span><br><span class="line">    <span class="comment"># split into qkv</span></span><br><span class="line">    qkv = np.split(x, <span class="number">3</span>, axis=-<span class="number">1</span>)  <span class="comment"># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span></span><br><span class="line">    <span class="keyword">if</span> kvcache:</span><br><span class="line">        <span class="comment"># qkv</span></span><br><span class="line">        new_q, new_k, new_v = qkv  <span class="comment"># new_q, new_k, new_v = [1, n_embd]</span></span><br><span class="line">        old_k, old_v = kvcache</span><br><span class="line">        k = np.vstack([old_k, new_k]) <span class="comment"># k = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span></span><br><span class="line">        v = np.vstack([old_v, new_v]) <span class="comment"># v = [n_seq, n_embd], where n_seq = prev_n_seq + 1</span></span><br><span class="line">        qkv = [new_q, k, v] <span class="comment"># new_q = [1, n_embd]</span></span><br><span class="line">    current_cache = [qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]]</span><br><span class="line">    <span class="comment"># split into heads</span></span><br><span class="line">    qkv_heads = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: np.split(x, n_head, axis=-<span class="number">1</span>), qkv))  <span class="comment"># [3, n_seq, n_embd] -&gt; [n_head, 3, n_seq, n_embd/n_head]</span></span><br><span class="line">    <span class="comment"># causal mask to hide future inputs from being attended to</span></span><br><span class="line">    <span class="keyword">if</span> kvcache:</span><br><span class="line">        causal_mask = np.zeros((<span class="number">1</span>, k.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        causal_mask = (<span class="number">1</span> - np.tri(x.shape[<span class="number">0</span>])) * -<span class="number">1e10</span>  <span class="comment"># [n_seq, n_seq]</span></span><br><span class="line">    <span class="comment"># perform attention over each head</span></span><br><span class="line">    out_heads = [attention(q, k, v, causal_mask) <span class="keyword">for</span> q, k, v <span class="keyword">in</span> <span class="built_in">zip</span>(*qkv_heads)]  <span class="comment"># [n_head, 3, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]</span></span><br><span class="line">    <span class="comment"># merge heads</span></span><br><span class="line">    x = np.hstack(out_heads)  <span class="comment"># [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="comment"># out projection</span></span><br><span class="line">    x = linear(x, **c_proj)  <span class="comment"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span></span><br><span class="line">    <span class="keyword">return</span> x, current_cache</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then we apply this KV-cached multi-head attention into the whole gpt2 model (cover all blocks)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kv_cached_gpt2</span>(<span class="params">inputs: <span class="built_in">list</span>[<span class="built_in">int</span>], kvcache, **params</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">list</span>[<span class="built_in">float</span>]]:</span><br><span class="line">    <span class="comment"># inputs: [sep_len], outputs: [sep_len, vocab_size]</span></span><br><span class="line">    <span class="comment"># provide arbitrary inputs and get output of the same length, with each element of the output indicating the probability of the next token.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> kvcache:</span><br><span class="line">        kvcache = [<span class="literal">None</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># cache already available, only send last token as input for predicting next token</span></span><br><span class="line">        inputs = [inputs[-<span class="number">1</span>]] <span class="comment"># we only feed one query into model to calculate attention</span></span><br><span class="line">    output, new_kvcache = Pseudo_Model(inputs, kvcache, **params)</span><br><span class="line">    <span class="keyword">return</span> output, new_kvcache</span><br><span class="line"></span><br><span class="line"><span class="comment"># inference with KV cache</span></span><br><span class="line">kvcache = <span class="literal">None</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_tokens_to_generate):  <span class="comment"># auto-regressive decode loop</span></span><br><span class="line">    logits, kvcache = kv_cached_gpt2(inputs, kvcache=kvcache, **params)  <span class="comment"># model forward pass</span></span><br><span class="line">    next_id = np.argmax(logits[-<span class="number">1</span>])  <span class="comment"># greedy sampling</span></span><br><span class="line">    inputs.append(<span class="built_in">int</span>(next_id))  <span class="comment"># append prediction to input</span></span><br><span class="line">generated_tokens = inputs[<span class="built_in">len</span>(inputs) - n_tokens_to_generate :]  <span class="comment"># only return generated ids</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3><span id="references">References:</span></h3><ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Mn_9W1nCFLo">https://www.youtube.com/watch?v=Mn_9W1nCFLo</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hkproj/pytorch-llama-notes/blob/main/LLaMA_Final.pdf">https://github.com/hkproj/pytorch-llama-notes/blob/main/LLaMA_Final.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://www.dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/">https://www.dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jaymody/picoGPT/pull/7/files">https://github.com/jaymody/picoGPT/pull/7/files</a></li>
<li><a target="_blank" rel="noopener" href="https://kipp.ly/transformer-inference-arithmetic/#kv-cache">https://kipp.ly/transformer-inference-arithmetic/#kv-cache</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@joaolages/kv-caching-explained-276520203249">https://medium.com/@joaolages/kv-caching-explained-276520203249</a></li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></li>
</ul>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/ichbinhandsome">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">GitHub</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/besser-46">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/12/24/Coding-Transformer-model-from-scratch/" rel="prev" title="Coding Transformer model from scratch">
      <i class="fa fa-chevron-left"></i> Coding Transformer model from scratch
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/01/07/MoE/" rel="next" title="MoE (Mix-of-Expert) Model">
      MoE (Mix-of-Expert) Model <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">References:</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruixiang Wang</p>
  <div class="site-description" itemprop="description">somthing interesting</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ichbinhandsome" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ichbinhandsome" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/besser-46" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;besser-46" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/ruixiang-wang-85769a199" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;ruixiang-wang-85769a199" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.kaggle.com/wangruixiang07" title="Kaggle → https:&#x2F;&#x2F;www.kaggle.com&#x2F;wangruixiang07" rel="noopener" target="_blank"><i class="fab fa-kaggle fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fas fa-biohazard"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruixiang Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bgBMKRcFSCM47e8KkwhkNhxA-MdYXbMMI","app_key":"gyzjWP17vkNtWyHEjWdY5c8Y","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":250,"height":500},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
