<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ichbinhandsome.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="This blog aims to implement some important ML and DL algorithms from scratch, which can help people get deeper understanding about how these algorithms work.  Classic Machine Learning algorithmsK-mean">
<meta property="og:type" content="article">
<meta property="og:title" content="Implementation of ML &amp; DL &amp; NLP algorithms (WIP)">
<meta property="og:url" content="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/index.html">
<meta property="og:site_name" content="Ruixiang&#39;s blog">
<meta property="og:description" content="This blog aims to implement some important ML and DL algorithms from scratch, which can help people get deeper understanding about how these algorithms work.  Classic Machine Learning algorithmsK-mean">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022009002.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261757405.jpeg">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072019388.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072025852.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072027841.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261845730.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212101170.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212149406.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022024946.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071937092.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071932599.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071928626.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071955958.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071939471.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072005801.png">
<meta property="article:published_time" content="2023-05-13T16:53:38.000Z">
<meta property="article:modified_time" content="2024-01-07T19:42:39.364Z">
<meta property="article:author" content="Ruixiang Wang">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png">

<link rel="canonical" href="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Implementation of ML & DL & NLP algorithms (WIP) | Ruixiang's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ruixiang's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">work harder, study better, do faster, become stronger</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruixiang Wang">
      <meta itemprop="description" content="somthing interesting">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruixiang's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Implementation of ML & DL & NLP algorithms (WIP)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-13 18:53:38" itemprop="dateCreated datePublished" datetime="2023-05-13T18:53:38+02:00">2023-05-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-01-07 20:42:39" itemprop="dateModified" datetime="2024-01-07T20:42:39+01:00">2024-01-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          
            <span id="/2023/05/13/ML_DL_algo/" class="post-meta-item leancloud_visitors" data-flag-title="Implementation of ML & DL & NLP algorithms (WIP)" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>This blog aims to implement some important ML and DL algorithms from scratch, which can help people get deeper understanding about how these algorithms work. </p>
<h2 id="Classic-Machine-Learning-algorithms"><a href="#Classic-Machine-Learning-algorithms" class="headerlink" title="Classic Machine Learning algorithms"></a>Classic Machine Learning algorithms</h2><h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><ul>
<li>Steps:<ol>
<li>Clusters the data into <em>k</em> groups where <em>k</em> is predefined.</li>
<li>Select <em>k</em> points at <em>random</em> as cluster centers.</li>
<li>Assign objects to their closest cluster center according to the <em>Euclidean distance</em> function.</li>
<li>Calculate the <em>centroid</em> or <em>mean</em> of all objects in each cluster.</li>
<li>Repeat steps 2, 3 and 4 until the same points are assigned to each cluster in consecutive rounds. <a id="more"></a></li>
</ol>
</li>
<li>Implementation:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_dist</span>(<span class="params">vector1, vector2</span>):</span></span><br><span class="line">    <span class="comment"># euclidian distance </span></span><br><span class="line">    eucl_dist = np.sqrt(np.sum((vector1 - vector2)**<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> eucl_dist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">k_means</span>(<span class="params">data, k</span>):</span></span><br><span class="line">    n_samples, n_shape = data.shape</span><br><span class="line">    <span class="comment"># random initialize centroids </span></span><br><span class="line">    centroid_index = np.random.choice(range(n_samples), k)</span><br><span class="line">    centroid_coords = data[centroid_index]</span><br><span class="line">    <span class="comment"># label of each data point</span></span><br><span class="line">    labels = [<span class="number">-1</span>] * n_samples</span><br><span class="line">    <span class="comment"># iterate until centroid no longer change</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            min_dist = float(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">            min_cluster_idx = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                dist = cal_dist(data[i], centroid_coords[j])</span><br><span class="line">                <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">                    min_dist = dist</span><br><span class="line">                    min_cluster_idx = k</span><br><span class="line">            <span class="comment"># update cluster index for each point</span></span><br><span class="line">            label[i] = min_cluster_idx</span><br><span class="line">        <span class="comment"># flag indicate if the centroid change of not: stop condition for while loop</span></span><br><span class="line">        centroid_update = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># recalculate the centroids </span></span><br><span class="line">        <span class="keyword">for</span> i, centroid <span class="keyword">in</span> enumerate(centroid_coords):</span><br><span class="line">            <span class="comment"># gather all data points for each cluster</span></span><br><span class="line">            cluster_points = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(n_samples):</span><br><span class="line">                <span class="keyword">if</span> label[k] == i:</span><br><span class="line">                    cluster_points.append(data[k])</span><br><span class="line">            cluster_points = np.concatenate([cluster_points], axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># calculate new centroid</span></span><br><span class="line">            new_centroid_coord = np.mean(cluster_points, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># see if the centroid change for each cluster</span></span><br><span class="line">            <span class="keyword">if</span> cal_dist(centroid, new_centroid_coord) &gt; <span class="number">1e-5</span>:</span><br><span class="line">                centroid_coords[i] = new_centroid_coord</span><br><span class="line">                centroid_update = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> centroid_update:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> labels</span><br><span class="line">                        </span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h3><p>NumPy implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://blog.51cto.com/u_15661962/5531479 </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainable weights and bias init</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_init</span>(<span class="params">n_features</span>):</span></span><br><span class="line">    <span class="comment"># n_features: size of each data point</span></span><br><span class="line">    w = np.zeros(<span class="number">1</span>, n_features)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"><span class="comment"># activation function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function: binary cross entropy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bce_loss</span>(<span class="params">pred, y, n</span>):</span></span><br><span class="line">    <span class="comment"># n is the number of samples</span></span><br><span class="line">    <span class="comment"># pred, y shape: [n, 1]</span></span><br><span class="line">    loss = (<span class="number">-1</span>/n) * (np.sum(y * np.log(pred) + (<span class="number">1</span>-y) * np.log(<span class="number">1</span>-pred)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span>(<span class="params">X, Y</span>):</span></span><br><span class="line">    <span class="comment"># X: input data</span></span><br><span class="line">    <span class="comment"># Y: labels</span></span><br><span class="line">    n_samples, n_features = X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]</span><br><span class="line">    w, b = weight_init(n_features)</span><br><span class="line">    <span class="comment"># hyper parameters</span></span><br><span class="line">    iteration = <span class="number">10</span></span><br><span class="line">    lr = <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iteration):</span><br><span class="line">        <span class="comment"># np.dot(x,y.T) == x @ y.T</span></span><br><span class="line">        pred = sigmoid(np.dot(X, w.T) + b)</span><br><span class="line">        loss = (pred, Y, n_samples)</span><br><span class="line">        <span class="comment"># calculate gradients</span></span><br><span class="line">        <span class="comment"># reference: https://laobadao.github.io/2017/10/27/logistic-cost/index.html</span></span><br><span class="line">        <span class="comment"># https://gist.github.com/golamSaroar/1a1ce33139cf77c37cd6bd123f0fa9cb</span></span><br><span class="line">        w_grad = (<span class="number">1</span>/n_samples) * (np.dot(X.T, (pred - Y.T).T))</span><br><span class="line">        b_grad = (<span class="number">1</span>/n_samples) * （np.sum(pred - Y.T)）</span><br><span class="line">        <span class="comment"># update w, b</span></span><br><span class="line">        w -= lr * w_grad</span><br><span class="line">        b -= lr * b_grad</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>

<h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>NumPy implementation for multiple linear regression (two or more features for each data point), simple linear regression (only one feature for each data point)<br><a target="_blank" rel="noopener" href="https://www.mvrlink.com/linear-regression-from-scratch-with-numpy/">reference</a>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean-squared error loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_loss</span>(<span class="params">y_pred, y</span>):</span></span><br><span class="line">    n = y.shape[<span class="number">0</span>] <span class="comment"># number of samples</span></span><br><span class="line">    loss = (<span class="number">1</span>/n) * np.sum((y - y_pred)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainable weights and bias init</span></span><br><span class="line"><span class="comment"># n_features &gt;= 2 for multiple linear regression</span></span><br><span class="line"><span class="comment"># n_features = 1 for simple linear regression</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_init</span>(<span class="params">n_features</span>):</span></span><br><span class="line">    w = np.zeros(<span class="number">1</span>, n_features)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span>(<span class="params">n_iteration, x, y, lr=<span class="number">1e-5</span></span>):</span></span><br><span class="line">    n_samples, n_features = x.shape[<span class="number">0</span>], x.shape[<span class="number">1</span>]</span><br><span class="line">    w, b = weight_init(n_features)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iteration):</span><br><span class="line">        y_pred = x @ w.T + b</span><br><span class="line">        mse_loss = (y_pred, y)</span><br><span class="line">        w_grad = (<span class="number">1</span> / n_samples) * np.dot(x.T, y_pred - y)</span><br><span class="line">        b_grad = (<span class="number">1</span> / n_samples) * np.sum(y - y_pred)</span><br><span class="line">        w -= lr * w_grad</span><br><span class="line">        b -= lr * b_grad</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>

<h3 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png" alt="activation_functions"></p>
<ul>
<li><p>sigmoid (value from 0 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>ReLU (from 0 to infinity)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
</li>
<li><p>tanh (from -1 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tahn</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (np.exp(x)-np.exp(-x)) / (np.exp(x) + np.exp(-x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Softmax (from 0 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute softmax values for each sets of scores in x.&quot;&quot;&quot;</span></span><br><span class="line">    e_x = np.exp(x - np.max(x))</span><br><span class="line">    <span class="keyword">return</span> e_x / e_x.sum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>GLU (Gated Linear Units) activation function<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022009002.png" alt="glu"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GLU</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_size</span>):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(in_size, in_size)</span><br><span class="line">        self.linear2 = nn.Linear(in_size, in_size)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.linear1(x) * self.linear2(x).sigmoid()</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261757405.jpeg" alt="loss_func"></p>
<ul>
<li><p>Binary Cross Entropy Loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_cross_entropy</span>(<span class="params">actual, predicted</span>):</span></span><br><span class="line">    sum_score = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(actual)):</span><br><span class="line">        sum_score += actual[i] * np.log(<span class="number">1e-10</span> + predicted[i])</span><br><span class="line">    mean_sum_score = <span class="number">-1.0</span> / len(actual) * sum_score</span><br></pre></td></tr></table></figure>
</li>
<li><p>Categorical Cross Entropy Loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical_cross_entropy</span>(<span class="params">actual, predicted</span>):</span></span><br><span class="line">    sum_score = <span class="number">0.0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(actual)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(actual[i])):</span><br><span class="line">            sum_score += actual[i][j] * np.log(<span class="number">1e-10</span> + predicted[i][j])</span><br><span class="line">    mean_sum_score = <span class="number">-1.0</span> /len(actual) * sum_score</span><br><span class="line">    <span class="keyword">return</span> mean_sum_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># class CrossEntropyLoss():</span></span><br><span class="line"><span class="comment">#     def forward(self, in_probs, labels):</span></span><br><span class="line"><span class="comment">#         self.in_probs = self.in_probs</span></span><br><span class="line"><span class="comment">#         self.labels = labels</span></span><br><span class="line"><span class="comment">#         return np.sum(-labels*np.log(self.in_probs))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     def backward(self):</span></span><br><span class="line"><span class="comment">#         return -self.labels/self.in_probs</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Evaluation-metrics"><a href="#Evaluation-metrics" class="headerlink" title="Evaluation metrics"></a>Evaluation metrics</h3><h4 id="Recall-Precision-F1-accuracy"><a href="#Recall-Precision-F1-accuracy" class="headerlink" title="Recall, Precision, F1, accuracy"></a>Recall, Precision, F1, accuracy</h4><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072019388.png" alt="rpf1"></p>
<h4 id="Marco-amp-Micro-average-recall-precision"><a href="#Marco-amp-Micro-average-recall-precision" class="headerlink" title="Marco &amp; Micro average recall/precision"></a>Marco &amp; Micro average recall/precision</h4><p>A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In macro averaging, we compute the performance for each class, and then average over classes. In micro averaging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and recall from that table.<br>In a multi-class classification setup, <strong>micro-average is preferable</strong> if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072025852.png" alt="micro_marco"><br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072027841.png" alt="micr0_marco_2"><br>Macro-averaged metrics are used when we want to evaluate systems performance across on different datasets.<br>Micro-averaged metrics should be used when the size of datasets are variable.</p>
<h2 id="Deep-Learning-algorithms"><a href="#Deep-Learning-algorithms" class="headerlink" title="Deep Learning algorithms"></a>Deep Learning algorithms</h2><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>Why normalization?<br>The phenomenon by which the distributions of internal nodes (neurons) of a neural network change is referred to as <strong>Internal Covariate Shift</strong>. And we want to avoid it because it makes training the network slower, as the neurons are forced to re-adjust drastically their weights in one direction or another because of drastic changes in the outputs of the previous layers.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261845730.png" alt="norm"></p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212101170.png" alt="batch_norm"></p>
<ul>
<li>PyTorch implementation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span>(<span class="params">x, gamma, beta, eps=<span class="number">1e-5</span></span>):</span></span><br><span class="line">    N, D = x.shape <span class="comment"># N is the batch size, D is the hidden size of each sample in batch</span></span><br><span class="line">    sample_mean = x.mean(axis=<span class="number">0</span>) <span class="comment"># shape [1, D]</span></span><br><span class="line">    sample_var = x.var(axis=<span class="number">0</span>) <span class="comment"># shape [1, D]</span></span><br><span class="line">    std = np.sqrt(sample_var + eps)</span><br><span class="line">    x_centered = x - sample_mean</span><br><span class="line">    x_norm = x_centered / std</span><br><span class="line">    out = gamma * x_norm + beta</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li>
<li>NumPy implementation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span>(<span class="params">x, scales,, bias, eps=<span class="number">1e-5</span></span>):</span></span><br><span class="line">  mean = x.mean(axis=<span class="number">0</span>)  <span class="comment"># Shape = (w, h, c)</span></span><br><span class="line">  var  = x.var(axis=<span class="number">0</span>)</span><br><span class="line">  std = np.sqrt(var+ eps) <span class="comment"># shape = (w, h, c)</span></span><br><span class="line">  <span class="comment"># eps is used to avoid divisions by zero</span></span><br><span class="line">  <span class="comment"># Compute the normalized input</span></span><br><span class="line">  x_norm = (x - mean) / std <span class="comment"># shape (batch, w, h, c)</span></span><br><span class="line">  <span class="comment"># Output = scale * x_norm + bias</span></span><br><span class="line">  <span class="keyword">if</span> scales <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      x_norm *= scales  <span class="comment"># Multiplication for scales</span></span><br><span class="line">  <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      x_norm += bias <span class="comment"># Add bias</span></span><br><span class="line">  <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212149406.png" alt="layer_norm"><br>Explanation:</p>
<blockquote>
<p>A well-known explanation of the success of LayerNorm is its re-centering and re-scaling invariance property. The former enables the model to be insensitive to shift noises on both inputs and weights, and the latter keeps the output representations intact when both inputs and weights are randomly scaled.</p>
</blockquote>
<ul>
<li><p>Numpy implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm</span>(<span class="params">x, g, b, eps: float = <span class="number">1e-5</span></span>):</span></span><br><span class="line">    mean = np.mean(x, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    variance = np.var(x, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    x = (x - mean) / np.sqrt(variance + eps)  <span class="comment"># normalize x to have mean=0 and var=1 over last axis</span></span><br><span class="line">    <span class="keyword">return</span> g * x + b  <span class="comment"># scale and offset with gamma/beta params</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Pytorch implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># feature : (batch_size, seq_len, hidden_dim)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-5</span></span>):</span></span><br><span class="line">        super(LayerNorma, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">farward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># (batch_size, seq_len, 1)</span></span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># (batch_size, seq_len, 1)</span></span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x-mean) / (std+self.eps) + self.b_2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022024946.png" alt="dropout"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dropout</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self. drop_prob</span>):</span></span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        self.dropout = (np.random.rand(*inputs.shape) &gt; self.dorp_prob)</span><br><span class="line">        <span class="keyword">return</span> np.where(self.dropout, inputs, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, grads</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.where(self.dropout, grads, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h3><h4 id="SGD-Stochastic-Gradient-Descent"><a href="#SGD-Stochastic-Gradient-Descent" class="headerlink" title="SGD (Stochastic Gradient Descent)"></a>SGD (Stochastic Gradient Descent)</h4><p>Advantages:</p>
<ol>
<li>Frequent updates of model parameters, converges in less time</li>
<li>Requires less memory as no need to store values of loss functions</li>
<li>May get new minima’s<br>Disadvantages:</li>
<li>High variance in model parameters</li>
<li>May shoot even after achieving global minima</li>
<li>To get the same convergence as gradient descent needs to slowly reduce the value of learning rate</li>
</ol>
<h4 id="Mini-Batch-Gradient-Descent-BGD"><a href="#Mini-Batch-Gradient-Descent-BGD" class="headerlink" title="Mini-Batch Gradient Descent (BGD)"></a>Mini-Batch Gradient Descent (BGD)</h4><p>Advantages:</p>
<ol>
<li>Frequently updates the model parameters and also has less variance</li>
<li>Requires medium amount of memory<br>All types of Gradient Descent have some challenges:</li>
<li>May get trapped at local minima</li>
<li>Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge</li>
<li>Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate</li>
</ol>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>Used in conjunction Stochastic Gradient Descent or Mini-Batch Gradient Descent, Momentum takes into account past gradients to smooth out the update.<br>This is seen in variable 𝑣, which is an exponentially weighted average of the gradient on previous steps. This results in minimizing oscillations and faster convergence.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071937092.png" alt="momentum"><br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071932599.png" alt="moment"><br>Compared with (minibatch) stochastic gradient descent the momentum method needs to maintain a set of auxiliary variables, i.e., velocity. It has the same shape as the gradients (and variables of the optimization problem). In the implementation below we call these variables states.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_momentum_states</span>(<span class="params">feature_dim</span>):</span></span><br><span class="line">    v_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span>(<span class="params">params, states, hyperparams</span>):</span></span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = hyperparams[<span class="string">&#x27;momentum&#x27;</span>] * v + p.grad</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4 id="Adagrad-Adaptative-Gradient"><a href="#Adagrad-Adaptative-Gradient" class="headerlink" title="Adagrad (Adaptative Gradient)"></a>Adagrad (Adaptative Gradient)</h4><p>Adagard optimizer changes the learning rate for each parameter at every time step. Particularly, it tends to assign higher learning rates to infrequent features, which ensures that the parameter updates rely less on frequency and more on relevance.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071928626.png" alt="adagrad"><br>Advantages:</p>
<ol>
<li>Learning rate changes for each training parameters</li>
<li>Don’t need to manually tune the learning rate</li>
<li>Able to train on sparse data<br>Disadvantages:</li>
<li>Computationally expensive as a need to calculate the second order derivative</li>
<li>The learning rate is always decreasing results in slow training</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adagrad_states</span>(<span class="params">feature_dim</span>):</span></span><br><span class="line">    s_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span>(<span class="params">params, states, hyperparams</span>):</span></span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            s[:] += torch.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / torch.sqrt(s + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071955958.png" alt="adadelta"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adadelta_states</span>(<span class="params">feature_dim</span>):</span></span><br><span class="line">    s_w, s_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    delta_w, delta_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span>(<span class="params">params, states, hyperparams</span>):</span></span><br><span class="line">    rho, eps = hyperparams[<span class="string">&#x27;rho&#x27;</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># In-place updates via [:]</span></span><br><span class="line">            s[:] = rho * s + (<span class="number">1</span> - rho) * torch.square(p.grad)</span><br><span class="line">            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad</span><br><span class="line">            p[:] -= g</span><br><span class="line">            delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4 id="RMSProp-Root-Mean-Square-Propagation"><a href="#RMSProp-Root-Mean-Square-Propagation" class="headerlink" title="RMSProp (Root Mean Square Propagation)"></a>RMSProp (Root Mean Square Propagation)</h4><p>RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale coefficients. RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique to adjust the coefficient-wise preconditioner. The learning rate needs to be scheduled by the experimenter in practice.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071939471.png" alt="RMSProp"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rmsprop_states</span>(<span class="params">feature_dim</span>):</span></span><br><span class="line">    s_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span>(<span class="params">params, states, hyperparams</span>):</span></span><br><span class="line">    gamma, eps = hyperparams[<span class="string">&#x27;gamma&#x27;</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            s[:] = gamma * s + (<span class="number">1</span> - gamma) * torch.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / torch.sqrt(s + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4 id="Adam-Adaptive-Moment-Estimation"><a href="#Adam-Adaptive-Moment-Estimation" class="headerlink" title="Adam (Adaptive Moment Estimation)"></a>Adam (Adaptive Moment Estimation)</h4><p>Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. Adam computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072005801.png" alt="adam"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_adam_states</span>(<span class="params">feature_dim</span>):</span></span><br><span class="line">    v_w, v_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    s_w, s_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span>(<span class="params">params, states, hyperparams</span>):</span></span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> zip(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad</span><br><span class="line">            s[:] = beta2 * s + (<span class="number">1</span> - beta2) * torch.square(p.grad)</span><br><span class="line">            v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr)</span><br><span class="line">                                                       + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>Note: <strong>AdamW</strong>(Adam with decoupled weight decay)</p>
<h2 id="NLP-algorithms"><a href="#NLP-algorithms" class="headerlink" title="NLP algorithms"></a>NLP algorithms</h2><h3 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h3><p>fastText is usually used to do classification tasks, it has two main improvements:</p>
<ul>
<li>used char-level n-gram as additional word embedding features</li>
<li>used hierarchical softmax to reduce computation cost<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastText</span>(<span class="params">nn.module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        super(FastText, self).__init__()</span><br><span class="line">        self.embedding_word = nn.Embedding(</span><br><span class="line">            config.vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.vocab_size<span class="number">-1</span></span><br><span class="line">        )</span><br><span class="line">        self.embedding_bigram = nn.Embedding(</span><br><span class="line">            config.ngram_vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.ngram_vocab_size<span class="number">-1</span></span><br><span class="line">        )</span><br><span class="line">        self.embedding_trigram = nn.Embedding(</span><br><span class="line">            config.ngram_vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.ngram_vocab_size<span class="number">-1</span></span><br><span class="line">        )</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout_rate)</span><br><span class="line">        self.fc1 = nn.Linear(config.embed_size * <span class="number">3</span>, config.hidden_size)</span><br><span class="line">        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param x: x[0] (words), x[1] (bigram), x[2] (trigram): all have shape [batch_size, seq_length] </span></span><br><span class="line"><span class="string">        :return: [batch_size, num_classes] classification task</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        embed_bow = self.embedding_word(x[<span class="number">0</span>])</span><br><span class="line">        embed_bigram = self.embedding_bigram(x[<span class="number">1</span>])</span><br><span class="line">        embed_trigram = self.embedding_trigram(x[<span class="number">2</span>])</span><br><span class="line">        input = torch.cat((embed_bow, embed_bigram, embed_trigram), <span class="number">-1</span>)</span><br><span class="line">        out = input.mean(dim=<span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wevolf/p/15195143.html">https://www.cnblogs.com/wevolf/p/15195143.html</a></li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">https://lilianweng.github.io/posts/2021-05-31-contrastive/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hkproj/pytorch-llama/tree/main">https://github.com/hkproj/pytorch-llama/tree/main</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.07467.pdf">https://arxiv.org/pdf/1910.07467.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6">https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6</a></li>
<li><a target="_blank" rel="noopener" href="https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum">https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.15596.pdf">https://arxiv.org/pdf/2211.15596.pdf</a></li>
</ul>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/ichbinhandsome">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">GitHub</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/besser-46">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/17/job-summary/" rel="prev" title="2021秋招回顾">
      <i class="fa fa-chevron-left"></i> 2021秋招回顾
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/10/22/Attention-Implementaion/" rel="next" title="Attention Implementaion">
      Attention Implementaion <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Classic-Machine-Learning-algorithms"><span class="nav-number">1.</span> <span class="nav-text">Classic Machine Learning algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means"><span class="nav-number">1.1.</span> <span class="nav-text">K-means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-regression"><span class="nav-number">1.2.</span> <span class="nav-text">Logistic regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression"><span class="nav-number">1.3.</span> <span class="nav-text">Linear Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Activation-Functions"><span class="nav-number">1.4.</span> <span class="nav-text">Activation Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-Functions"><span class="nav-number">1.5.</span> <span class="nav-text">Loss Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation-metrics"><span class="nav-number">1.6.</span> <span class="nav-text">Evaluation metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recall-Precision-F1-accuracy"><span class="nav-number">1.6.1.</span> <span class="nav-text">Recall, Precision, F1, accuracy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Marco-amp-Micro-average-recall-precision"><span class="nav-number">1.6.2.</span> <span class="nav-text">Marco &amp; Micro average recall&#x2F;precision</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning-algorithms"><span class="nav-number">2.</span> <span class="nav-text">Deep Learning algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">2.1.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Layer-Normalization"><span class="nav-number">2.2.</span> <span class="nav-text">Layer Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">2.3.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimizers"><span class="nav-number">2.4.</span> <span class="nav-text">Optimizers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SGD-Stochastic-Gradient-Descent"><span class="nav-number">2.4.1.</span> <span class="nav-text">SGD (Stochastic Gradient Descent)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mini-Batch-Gradient-Descent-BGD"><span class="nav-number">2.4.2.</span> <span class="nav-text">Mini-Batch Gradient Descent (BGD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Momentum"><span class="nav-number">2.4.3.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adagrad-Adaptative-Gradient"><span class="nav-number">2.4.4.</span> <span class="nav-text">Adagrad (Adaptative Gradient)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adadelta"><span class="nav-number">2.4.5.</span> <span class="nav-text">Adadelta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RMSProp-Root-Mean-Square-Propagation"><span class="nav-number">2.4.6.</span> <span class="nav-text">RMSProp (Root Mean Square Propagation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam-Adaptive-Moment-Estimation"><span class="nav-number">2.4.7.</span> <span class="nav-text">Adam (Adaptive Moment Estimation)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP-algorithms"><span class="nav-number">3.</span> <span class="nav-text">NLP algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FastText"><span class="nav-number">3.1.</span> <span class="nav-text">FastText</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruixiang Wang</p>
  <div class="site-description" itemprop="description">somthing interesting</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ichbinhandsome" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ichbinhandsome" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/besser-46" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;besser-46" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/ruixiang-wang-85769a199" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;ruixiang-wang-85769a199" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.kaggle.com/wangruixiang07" title="Kaggle → https:&#x2F;&#x2F;www.kaggle.com&#x2F;wangruixiang07" rel="noopener" target="_blank"><i class="fab fa-kaggle fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fas fa-biohazard"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruixiang Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bgBMKRcFSCM47e8KkwhkNhxA-MdYXbMMI","app_key":"gyzjWP17vkNtWyHEjWdY5c8Y","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":250,"height":500},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
