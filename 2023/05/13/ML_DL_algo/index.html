<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ichbinhandsome.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="This blog aims to implement some important Machine Learning, Deep Learning, Natural Language Processing, Large Language Models related algorithms from scratch, which can help people get deeper underst">
<meta property="og:type" content="article">
<meta property="og:title" content="Implementation of ML &amp; DL &amp; NLP algorithms (WIP)">
<meta property="og:url" content="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/index.html">
<meta property="og:site_name" content="Ruixiang&#39;s blog">
<meta property="og:description" content="This blog aims to implement some important Machine Learning, Deep Learning, Natural Language Processing, Large Language Models related algorithms from scratch, which can help people get deeper underst">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022009002.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402040009042.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261757405.jpeg">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202403210248190.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072019388.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072025852.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072027841.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041721113.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041730706.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041813997.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041816809.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822903.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822446.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041843225.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041856323.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041959054.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042007454.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042051887.png">
<meta property="og:image" content="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/!%5B%5D(https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041950356.png)">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261845730.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212101170.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212149406.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022024946.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071937092.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071932599.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071928626.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071955958.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071939471.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072005801.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031604097.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031620531.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401312314722.png">
<meta property="article:published_time" content="2023-05-13T16:53:38.000Z">
<meta property="article:modified_time" content="2024-03-23T01:05:38.280Z">
<meta property="article:author" content="Ruixiang Wang">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png">

<link rel="canonical" href="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Implementation of ML & DL & NLP algorithms (WIP) | Ruixiang's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ruixiang's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">work harder, study better, do faster, become stronger</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruixiang Wang">
      <meta itemprop="description" content="somthing interesting">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruixiang's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Implementation of ML & DL & NLP algorithms (WIP)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-13 18:53:38" itemprop="dateCreated datePublished" datetime="2023-05-13T18:53:38+02:00">2023-05-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-03-23 02:05:38" itemprop="dateModified" datetime="2024-03-23T02:05:38+01:00">2024-03-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          
            <span id="/2023/05/13/ML_DL_algo/" class="post-meta-item leancloud_visitors" data-flag-title="Implementation of ML & DL & NLP algorithms (WIP)" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>This blog aims to implement some important Machine Learning, Deep Learning, Natural Language Processing, Large Language Models related algorithms from scratch, which can help people get deeper understanding about how these algorithms work in real code. </p>
<ul>
<li><a href="#classic-machine-learning-algorithms">Classic Machine Learning algorithms</a><ul>
<li><a href="#k-means">K-means</a></li>
<li><a href="#knn-implementation">KNN implementation</a></li>
<li><a href="#logistic-regression">Logistic regression</a></li>
<li><a href="#linear-regression">Linear Regression</a></li>
<li><a href="#activation-functions">Activation Functions</a></li>
<li><a href="#loss-functions">Loss Functions</a></li>
<li><a href="#evaluation-metrics">Evaluation metrics</a><ul>
<li><a href="#recall-precision-f1-accuracy">Recall, Precision, F1, accuracy</a></li>
<li><a href="#marco--micro-average-recallprecision">Marco &amp; Micro average recall/precision</a></li>
<li><a href="#roc-and-auc">ROC and AUC</a></li>
<li><a href="#bleu-score">BLEU score</a></li>
<li><a href="#rouge-score">Rouge score</a></li>
<li><a href="#perplexity">Perplexity</a></li>
<li><a href="#mrr-map-ndcg">MRR, MAP, NDCG</a></li>
</ul>
</li>
<li><a href="#cross-validation">Cross Validation</a></li>
</ul>
</li>
<li><a href="#deep-learning-algorithms">Deep Learning algorithms</a><ul>
<li><a href="#batch-normalization">Batch Normalization</a></li>
<li><a href="#layer-normalization">Layer Normalization</a></li>
<li><a href="#dropout">Dropout</a></li>
<li><a href="#optimizers">Optimizers</a><ul>
<li><a href="#sgd-stochastic-gradient-descent">SGD (Stochastic Gradient Descent)</a></li>
<li><a href="#mini-batch-gradient-descent-bgd">Mini-Batch Gradient Descent (BGD)</a></li>
<li><a href="#momentum">Momentum</a></li>
<li><a href="#adagrad-adaptative-gradient">Adagrad (Adaptative Gradient)</a></li>
<li><a href="#adadelta">Adadelta</a></li>
<li><a href="#rmsprop-root-mean-square-propagation">RMSProp (Root Mean Square Propagation)</a></li>
<li><a href="#adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#cv-algorithms">CV algorithms</a><ul>
<li><a href="#cnn">CNN</a></li>
<li><a href="#patch-embedding">Patch Embedding</a></li>
</ul>
</li>
<li><a href="#nlp-algorithms">NLP algorithms</a><ul>
<li><a href="#basic-preprocessing">Basic Preprocessing</a></li>
<li><a href="#cosine-similarity">Cosine similarity</a></li>
<li><a href="#tf-idf">TF-IDF</a></li>
<li><a href="#fasttext">FastText</a></li>
<li><a href="#rnn-implementation">RNN implementation</a></li>
</ul>
</li>
<li><a href="#llm-algorithms">LLM algorithms</a><ul>
<li><a href="#lora-low-rank-adaption-implementation">LoRA (Low Rank Adaption) Implementation</a></li>
<li><a href="#autoregressive-decoding">Autoregressive decoding</a></li>
<li><a href="#speculative-decoding">Speculative Decoding</a></li>
</ul>
</li>
<li><a href="#mathematics">Mathematics</a><ul>
<li><a href="#different-distributions">Different distributions</a></li>
</ul>
</li>
<li><a href="#references">References</a><span id="more"></span>

</li>
</ul>
<h2><span id="classic-machine-learning-algorithms">Classic Machine Learning algorithms</span></h2><h3><span id="k-means">K-means</span></h3><ul>
<li><p>Steps:</p>
<ol>
<li>Clusters the data into <em>k</em> groups where <em>k</em> is predefined.</li>
<li>Select <em>k</em> points at <em>random</em> as cluster centers.</li>
<li>Assign objects to their closest cluster center according to the <em>Euclidean distance</em> function.</li>
<li>Calculate the <em>centroid</em> or <em>mean</em> of all objects in each cluster.</li>
<li>Repeat steps 2, 3 and 4 until the same points are assigned to each cluster in consecutive rounds. </li>
</ol>
</li>
<li><p>Implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_dist</span>(<span class="params">vector1, vector2</span>):</span><br><span class="line">    <span class="comment"># euclidian distance </span></span><br><span class="line">    eucl_dist = np.sqrt(np.<span class="built_in">sum</span>((vector1 - vector2)**<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> eucl_dist</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">k_means</span>(<span class="params">data, k</span>):</span><br><span class="line">    n_samples, n_shape = data.shape</span><br><span class="line">    <span class="comment"># random initialize centroids </span></span><br><span class="line">    centroid_index = np.random.choice(<span class="built_in">range</span>(n_samples), k)</span><br><span class="line">    centroid_coords = data[centroid_index]</span><br><span class="line">    <span class="comment"># label of each data point</span></span><br><span class="line">    labels = [-<span class="number">1</span>] * n_samples</span><br><span class="line">    <span class="comment"># iterate until centroid no longer change</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">            min_dist = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">            min_cluster_idx = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                dist = cal_dist(data[i], centroid_coords[j])</span><br><span class="line">                <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">                    min_dist = dist</span><br><span class="line">                    min_cluster_idx = k</span><br><span class="line">            <span class="comment"># update cluster index for each point</span></span><br><span class="line">            label[i] = min_cluster_idx</span><br><span class="line">        <span class="comment"># flag indicate if the centroid change of not: stop condition for while loop</span></span><br><span class="line">        centroid_update = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># recalculate the centroids </span></span><br><span class="line">        <span class="keyword">for</span> i, centroid <span class="keyword">in</span> <span class="built_in">enumerate</span>(centroid_coords):</span><br><span class="line">            <span class="comment"># gather all data points for each cluster</span></span><br><span class="line">            cluster_points = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">                <span class="keyword">if</span> label[k] == i:</span><br><span class="line">                    cluster_points.append(data[k])</span><br><span class="line">            cluster_points = np.concatenate([cluster_points], axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># calculate new centroid</span></span><br><span class="line">            new_centroid_coord = np.mean(cluster_points, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># see if the centroid change for each cluster</span></span><br><span class="line">            <span class="keyword">if</span> cal_dist(centroid, new_centroid_coord) &gt; <span class="number">1e-5</span>:</span><br><span class="line">                centroid_coords[i] = new_centroid_coord</span><br><span class="line">                centroid_update = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> centroid_update:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> labels</span><br><span class="line">                        </span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3><span id="knn-implementation">KNN implementation</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">kNN</span>():</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;k-Nearest Neighbours&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Initialise</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k=<span class="number">3</span>, metric=<span class="string">&#x27;euclidean&#x27;</span>, p=<span class="literal">None</span></span>):</span><br><span class="line">        self.k = k</span><br><span class="line">        self.metric = metric</span><br><span class="line">        self.p = p</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Euclidean distance (l2 norm)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">euclidean</span>(<span class="params">self, v1, v2</span>):</span><br><span class="line">        <span class="keyword">return</span> np.sqrt(np.<span class="built_in">sum</span>((v1-v2)**<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Manhattan distance (l1 norm)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">manhattan</span>(<span class="params">self, v1, v2</span>):</span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(v1-v2))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Minkowski distance (lp norm)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minkowski</span>(<span class="params">self, v1, v2, p=<span class="number">2</span></span>):</span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(v1-v2)**p)**(<span class="number">1</span>/p)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Store train set</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X_train, y_train</span>):</span><br><span class="line">        self.X_train = X_train</span><br><span class="line">        self.y_train = y_train</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Make predictions</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X_test</span>):</span><br><span class="line">        preds = []</span><br><span class="line">        <span class="comment"># Loop over rows in test set</span></span><br><span class="line">        <span class="keyword">for</span> test_row <span class="keyword">in</span> X_test:</span><br><span class="line">            nearest_neighbours = self.get_neighbours(test_row)</span><br><span class="line">						<span class="comment"># majority class prediction of a test instance.</span></span><br><span class="line">            majority = stats.mode(nearest_neighbours)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">            preds.append(majority)</span><br><span class="line">        <span class="keyword">return</span> np.array(preds)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get nearest neighbours</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_neighbours</span>(<span class="params">self, test_row</span>):</span><br><span class="line">        distances = <span class="built_in">list</span>()</span><br><span class="line">        <span class="comment"># Calculate distance to all points in X_train</span></span><br><span class="line">        <span class="keyword">for</span> (train_row, train_class) <span class="keyword">in</span> <span class="built_in">zip</span>(self.X_train, self.y_train):</span><br><span class="line">            <span class="keyword">if</span> self.metric==<span class="string">&#x27;euclidean&#x27;</span>:</span><br><span class="line">                dist = self.euclidean(train_row, test_row)</span><br><span class="line">            <span class="keyword">elif</span> self.metric==<span class="string">&#x27;manhattan&#x27;</span>:</span><br><span class="line">                dist = self.manhattan(train_row, test_row)</span><br><span class="line">            <span class="keyword">elif</span> self.metric==<span class="string">&#x27;minkowski&#x27;</span>:</span><br><span class="line">                dist = self.minkowski(train_row, test_row, self.p)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NameError(<span class="string">&#x27;Supported metrics are euclidean, manhattan and minkowski&#x27;</span>)</span><br><span class="line">            distances.append((dist, train_class))</span><br><span class="line">        <span class="comment"># Sort distances</span></span><br><span class="line">        distances.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># Identify k nearest neighbours</span></span><br><span class="line">        neighbours = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.k):</span><br><span class="line">            neighbours.append(distances[i][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> neighbours</span><br></pre></td></tr></table></figure>

<h3><span id="logistic-regression">Logistic regression</span></h3><p>NumPy implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://blog.51cto.com/u_15661962/5531479 </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainable weights and bias init</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weight_init</span>(<span class="params">n_features</span>):</span><br><span class="line">    <span class="comment"># n_features: size of each data point</span></span><br><span class="line">    w = np.zeros(<span class="number">1</span>, n_features)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"><span class="comment"># activation function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function: binary cross entropy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bce_loss</span>(<span class="params">pred, y, n</span>):</span><br><span class="line">    <span class="comment"># n is the number of samples</span></span><br><span class="line">    <span class="comment"># pred, y shape: [n, 1]</span></span><br><span class="line">    loss = (-<span class="number">1</span>/n) * (np.<span class="built_in">sum</span>(y * np.log(pred) + (<span class="number">1</span>-y) * np.log(<span class="number">1</span>-pred)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_regression</span>(<span class="params">X, Y</span>):</span><br><span class="line">    <span class="comment"># X: input data</span></span><br><span class="line">    <span class="comment"># Y: labels</span></span><br><span class="line">    n_samples, n_features = X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]</span><br><span class="line">    w, b = weight_init(n_features)</span><br><span class="line">    <span class="comment"># hyper parameters</span></span><br><span class="line">    iteration = <span class="number">10</span></span><br><span class="line">    lr = <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">        <span class="comment"># np.dot(x,y.T) == x @ y.T</span></span><br><span class="line">        pred = sigmoid(np.dot(X, w.T) + b)</span><br><span class="line">        loss = (pred, Y, n_samples)</span><br><span class="line">        <span class="comment"># calculate gradients</span></span><br><span class="line">        <span class="comment"># reference: https://laobadao.github.io/2017/10/27/logistic-cost/index.html</span></span><br><span class="line">        <span class="comment"># https://gist.github.com/golamSaroar/1a1ce33139cf77c37cd6bd123f0fa9cb</span></span><br><span class="line">        w_grad = (<span class="number">1</span>/n_samples) * (np.dot(X.T, (pred - Y.T).T))</span><br><span class="line">        b_grad = (<span class="number">1</span>/n_samples) * （np.<span class="built_in">sum</span>(pred - Y.T)）</span><br><span class="line">        <span class="comment"># update w, b</span></span><br><span class="line">        w -= lr * w_grad</span><br><span class="line">        b -= lr * b_grad</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>

<h3><span id="linear-regression">Linear Regression</span></h3><p>NumPy implementation for multiple linear regression (two or more features for each data point), simple linear regression (only one feature for each data point)<br><a target="_blank" rel="noopener" href="https://www.mvrlink.com/linear-regression-from-scratch-with-numpy/">reference</a>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean-squared error loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_pred, y</span>):</span><br><span class="line">    n = y.shape[<span class="number">0</span>] <span class="comment"># number of samples</span></span><br><span class="line">    loss = (<span class="number">1</span>/n) * np.<span class="built_in">sum</span>((y - y_pred)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainable weights and bias init</span></span><br><span class="line"><span class="comment"># n_features &gt;= 2 for multiple linear regression</span></span><br><span class="line"><span class="comment"># n_features = 1 for simple linear regression</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weight_init</span>(<span class="params">n_features</span>):</span><br><span class="line">    w = np.zeros(<span class="number">1</span>, n_features)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">n_iteration, x, y, lr=<span class="number">1e-5</span></span>):</span><br><span class="line">    n_samples, n_features = x.shape[<span class="number">0</span>], x.shape[<span class="number">1</span>]</span><br><span class="line">    w, b = weight_init(n_features)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_iteration):</span><br><span class="line">        y_pred = x @ w.T + b</span><br><span class="line">        mse_loss = (y_pred, y)</span><br><span class="line">        w_grad = (<span class="number">1</span> / n_samples) * np.dot(x.T, y_pred - y)</span><br><span class="line">        b_grad = (<span class="number">1</span> / n_samples) * np.<span class="built_in">sum</span>(y - y_pred)</span><br><span class="line">        w -= lr * w_grad</span><br><span class="line">        b -= lr * b_grad</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>

<h3><span id="activation-functions">Activation Functions</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png" alt="activation_functions"></p>
<ul>
<li><p>sigmoid (value from 0 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>ReLU (from 0 to infinity)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
</li>
<li><p>Leaky ReLU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, alpha=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the Leaky ReLU activation.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(alpha * x, x)</span><br></pre></td></tr></table></figure>
</li>
<li><p>tanh (from -1 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tahn</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> (np.exp(x)-np.exp(-x)) / (np.exp(x) + np.exp(-x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Softmax (from 0 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># assumes x is a vector</span></span><br><span class="line">    x = x - np.<span class="built_in">max</span>(x) <span class="comment"># to avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.<span class="built_in">sum</span>(np.exp(x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>GLU (Gated Linear Units) activation function<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022009002.png" alt="glu"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GLU</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(in_size, in_size)</span><br><span class="line">        self.linear2 = nn.Linear(in_size, in_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear1(x) * self.linear2(x).sigmoid()</span><br></pre></td></tr></table></figure></li>
<li><p>Sigmoid Linear Unit (SiLU/swish) activation function<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402040009042.png" alt="silu"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">silu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the SiLU activation for numpy arrays.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> x / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>

<h3><span id="loss-functions">Loss Functions</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261757405.jpeg" alt="loss_func"></p>
<ul>
<li><p>Binary Cross Entropy Loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">binary_cross_entropy</span>(<span class="params">actual, predicted</span>):</span><br><span class="line">    sum_score = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(actual)):</span><br><span class="line">        sum_score += actual[i] * np.log(<span class="number">1e-10</span> + predicted[i])</span><br><span class="line">    mean_sum_score = -<span class="number">1.0</span> / <span class="built_in">len</span>(actual) * sum_score</span><br></pre></td></tr></table></figure>
</li>
<li><p>Categorical Cross Entropy Loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">categorical_cross_entropy</span>(<span class="params">actual, predicted</span>):</span><br><span class="line">    sum_score = <span class="number">0.0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(actual)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(actual[i])):</span><br><span class="line">            sum_score += actual[i][j] * np.log(<span class="number">1e-10</span> + predicted[i][j])</span><br><span class="line">    mean_sum_score = -<span class="number">1.0</span> /<span class="built_in">len</span>(actual) * sum_score</span><br><span class="line">    <span class="keyword">return</span> mean_sum_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># class CrossEntropyLoss():</span></span><br><span class="line"><span class="comment">#     def forward(self, in_probs, labels):</span></span><br><span class="line"><span class="comment">#         self.in_probs = self.in_probs</span></span><br><span class="line"><span class="comment">#         self.labels = labels</span></span><br><span class="line"><span class="comment">#         return np.sum(-labels*np.log(self.in_probs))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     def backward(self):</span></span><br><span class="line"><span class="comment">#         return -self.labels/self.in_probs</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>KL divergence<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202403210248190.png" alt="kl_diver"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">KL</span>(<span class="params">P,Q</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    Epsilon is used here to avoid conditional code for</span></span><br><span class="line"><span class="string">    checking that neither P nor Q is equal to 0. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    epsilon = <span class="number">0.00001</span></span><br><span class="line">    P = P+epsilon</span><br><span class="line">    Q = Q+epsilon</span><br><span class="line">    divergence = np.<span class="built_in">sum</span>(P*np.log(P/Q))</span><br><span class="line">    <span class="keyword">return</span> divergence</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3><span id="evaluation-metrics">Evaluation metrics</span></h3><h4><span id="recall-precision-f1-accuracy">Recall, Precision, F1, accuracy</span></h4><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072019388.png" alt="rpf1"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">ground_truth = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>] </span><br><span class="line">prediction   = [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.05</span>, <span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.66</span>, <span class="number">0.3</span>,  <span class="number">0.2</span>, <span class="number">0.85</span>, <span class="number">0.15</span>, <span class="number">0.99</span>] </span><br><span class="line">thresholds    = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>,  <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.9</span>, <span class="number">0.99</span>, <span class="number">1.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#True Positive</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">true_positive</span>(<span class="params">ground_truth, prediction</span>):</span><br><span class="line">    tp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> gt, pred <span class="keyword">in</span> <span class="built_in">zip</span>(ground_truth, prediction):</span><br><span class="line">        <span class="keyword">if</span> gt == <span class="number">1</span> <span class="keyword">and</span> pred == <span class="number">1</span>:</span><br><span class="line">            tp +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tp</span><br><span class="line"></span><br><span class="line"><span class="comment">#True Negative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">true_negative</span>(<span class="params">ground_truth, prediction</span>):</span><br><span class="line">    tn = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> gt, pred <span class="keyword">in</span> <span class="built_in">zip</span>(ground_truth, prediction):</span><br><span class="line">        <span class="keyword">if</span> gt == <span class="number">0</span> <span class="keyword">and</span> pred == <span class="number">0</span>:</span><br><span class="line">            tn +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> tn</span><br><span class="line"></span><br><span class="line"><span class="comment">#False Positive</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">false_positive</span>(<span class="params">ground_truth, prediction</span>):</span><br><span class="line">    fp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> gt, pred <span class="keyword">in</span> <span class="built_in">zip</span>(ground_truth, prediction):</span><br><span class="line">        <span class="keyword">if</span> gt == <span class="number">0</span> <span class="keyword">and</span> pred == <span class="number">1</span>:</span><br><span class="line">            fp +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> fp</span><br><span class="line"></span><br><span class="line"><span class="comment">#False Negative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">false_negative</span>(<span class="params">ground_truth, prediction</span>):</span><br><span class="line">    fn = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> gt, pred <span class="keyword">in</span> <span class="built_in">zip</span>(ground_truth, prediction):</span><br><span class="line">        <span class="keyword">if</span> gt == <span class="number">1</span> <span class="keyword">and</span> pred == <span class="number">0</span>:</span><br><span class="line">            fn +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> fn</span><br><span class="line"></span><br><span class="line"><span class="comment"># True Positive Rate (Sensitivity or Recall) = TP/ (TP + FN)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tpr</span>(<span class="params">ground_truth, prediction</span>):</span><br><span class="line">    tp = true_positive(ground_truth, prediction)  </span><br><span class="line">    fn = false_negative(ground_truth, prediction)  </span><br><span class="line">    pr = tp/ (tp + fn)  </span><br><span class="line">    <span class="keyword">return</span> pr</span><br><span class="line"></span><br><span class="line"><span class="comment"># False Positive Rate (Sensitivity or Recall) = FP/ (TN + FP)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fpr</span>(<span class="params">ground_truth, prediction</span>):</span><br><span class="line">    fp = false_positive(ground_truth, prediction)  </span><br><span class="line">    tn = true_negative(ground_truth, prediction)  </span><br><span class="line">    fr = fp/ (tn + fp)  </span><br><span class="line">    <span class="keyword">return</span> fr</span><br><span class="line"></span><br><span class="line"><span class="comment"># ROC and AUC</span></span><br><span class="line">true_positive_rate = []</span><br><span class="line">false_poitive_rate = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> thresholds:  </span><br><span class="line">    <span class="comment">#calculate predictions for threshold  </span></span><br><span class="line">    value_pred = [<span class="number">1</span> <span class="keyword">if</span> x &gt;= threshold <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> prediction]   </span><br><span class="line">    value_tpr = tpr(ground_truth, value_pred)   </span><br><span class="line">    value_fpr = fpr(ground_truth, value_pred)  </span><br><span class="line">    true_positive_rate.append(value_tpr)  </span><br><span class="line">    false_positive_rate.append(value_fpr)</span><br><span class="line"><span class="comment"># ROC curve: X-axis =&gt; False Positive Rate (FPR), Y-axis =&gt; True Positive Rate (TPR) </span></span><br><span class="line"><span class="comment"># AUC : area under ROC curve</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_roc_auc_score</span>(<span class="params">true_positive_rate, false_positive_rate</span>):</span><br><span class="line">    tpr, fpr = true_positive_rate, false_positive_rate</span><br><span class="line">    <span class="comment"># compute AUC using the trapezoidal rule;</span></span><br><span class="line">    <span class="comment"># appending an extra 0 is just to ensure the length matches</span></span><br><span class="line">    zero = np.array([<span class="number">0</span>])</span><br><span class="line">    tpr_diff = np.hstack((np.diff(tpr), zero))</span><br><span class="line">    fpr_diff = np.hstack((np.diff(fpr), zero))</span><br><span class="line">    auc = np.dot(tpr, fpr_diff) + np.dot(tpr_diff, fpr_diff) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> auc</span><br></pre></td></tr></table></figure>
<h4><span id="marco-amp-micro-average-recallprecision">Marco &amp; Micro average recall/precision</span></h4><p>A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In macro averaging, we compute the performance for each class, and then average over classes. In micro averaging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and recall from that table.<br>In a multi-class classification setup, <strong>micro-average is preferable</strong> if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072025852.png" alt="micro_marco"><br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072027841.png" alt="micr0_marco_2"><br>Macro-averaged metrics are used when we want to evaluate systems performance across on different datasets.<br>Micro-averaged metrics should be used when the size of datasets are variable.</p>
<h4><span id="roc-and-auc">ROC and AUC</span></h4><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041721113.png" alt="roc"></p>
<p>A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values. The ROC curve is the plot of the <em>true positive rate (TPR)</em> against the <em>false positive rate (FPR)</em> at each threshold setting.</p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041730706.png" alt="tpr_fpr"></p>
<p>AUC stands for “Area under the ROC Curve.” That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.</p>
<h4><span id="bleu-score">BLEU score</span></h4><p>BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. BLEU’s output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts.</p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041813997.png" alt="bleu"></p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041816809.png" alt="belu_cal"></p>
<h4><span id="rouge-score">Rouge score</span></h4><p>ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.</p>
<ul>
<li>ROUGE-N: Overlap of n-grams between the system and reference summaries.</li>
<li>ROUGE-L: Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822903.png" alt="rouge"></p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822446.png" alt="rouge_p_r"></p>
<h4><span id="perplexity">Perplexity</span></h4><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041843225.png" alt="Perplexity"></p>
<p>Perplexity (PPL) is one of the most common metrics for evaluating language models. Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. Intuitively, it can be thought of as an evaluation of the model’s ability to predict uniformly among the set of specified tokens in a corpus. The lower perplexity means the model has better performance on the specific corpus.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041856323.png" alt="log_ppl"></p>
<h4><span id="mrr-map-ndcg">MRR, MAP, NDCG</span></h4><ul>
<li><p>MRR<br>Mean Reciprocal Rank (MRR) is one of the metrics that help evaluate the quality of recommendation and information retrieval systems. Mean Reciprocal Rank (MRR) defines how good a retrieval system is to return a relevant result as the top result. To calculate the MRR, multiple queries are invoked. If all of these queries return a relevant document at the top, the score will be 1.0.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041959054.png" alt="mrr"></p>
</li>
<li><p>MAP<br>The disadvantage of MRR is that only the top document is considered. Other use cases like comparative questions in Question Answering scenarios require multiple relevant documents. Mean Average Precision (MAP) addresses this shortcoming by considering the order of the returned items. To calculate this score, three steps are taken:</p>
<ol>
<li>Calculate precision</li>
<li>Calculate average precision for K</li>
<li>Calculate mean average precision for sample of queries<br>As for MRR, multiple queries are run. Instead of calculating the Recall, Precision is computed by using relevant and non-relevant documents from the returned lists (not the complete dataset).<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042007454.png" alt="map"></li>
</ol>
</li>
</ul>
<p>Average Precision (AP) is a widely used metric in information retrieval that quantifies the quality of a retrieval system’s ranked results for a single query. Unlike precision, which considers the entire ranked list of retrieved items, AP focuses on how well relevant items are ranked within that list. It measures the area under the precision-recall curve for a single query.<br>AP is calculated as follows:</p>
<ol>
<li>First, consider a query that retrieves a set of items.</li>
<li>Compute the precision for each position where a relevant item is retrieved in the ranked results list.</li>
<li>Average these precision values, but only for the positions where relevant items were retrieved.</li>
</ol>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042051887.png" alt="ap"></p>
<ul>
<li>CG, DCG, NDCG<br>Discounted cumulative gain (DCG) is a measure of ranking quality which is often presented in its normalized and more easily interpretable form, known as Normalized DCG (nDCG or NDCG). In information retrieval, NDCG is often used to measure effectiveness of search engine algorithms and related applications. </li>
</ul>
<p><img src="!%5B%5D(https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041950356.png)" alt="ndcg"></p>
<p>nDCG implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ndcg</span>(<span class="params">golden, current, n = -<span class="number">1</span></span>):</span><br><span class="line">    log2_table = np.log2(np.arange(<span class="number">2</span>, <span class="number">102</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dcg_at_n</span>(<span class="params">rel, n</span>):</span><br><span class="line">        rel = np.asfarray(rel)[:n]</span><br><span class="line">        dcg = np.<span class="built_in">sum</span>(np.divide(np.power(<span class="number">2</span>, rel) - <span class="number">1</span>, log2_table[:rel.shape[<span class="number">0</span>]]))</span><br><span class="line">        <span class="keyword">return</span> dcg</span><br><span class="line"></span><br><span class="line">    ndcgs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(current)):</span><br><span class="line">        k = <span class="built_in">len</span>(current[i]) <span class="keyword">if</span> n == -<span class="number">1</span> <span class="keyword">else</span> n</span><br><span class="line">        idcg = dcg_at_n(<span class="built_in">sorted</span>(golden[i], reverse=<span class="literal">True</span>), n=k)</span><br><span class="line">        dcg = dcg_at_n(current[i], n=k)</span><br><span class="line">        tmp_ndcg = <span class="number">0</span> <span class="keyword">if</span> idcg == <span class="number">0</span> <span class="keyword">else</span> dcg / idcg</span><br><span class="line">        ndcgs.append(tmp_ndcg)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.</span> <span class="keyword">if</span> <span class="built_in">len</span>(ndcgs) == <span class="number">0</span> <span class="keyword">else</span> <span class="built_in">sum</span>(ndcgs) / (<span class="built_in">len</span>(ndcgs))</span><br></pre></td></tr></table></figure>

<h3><span id="cross-validation">Cross Validation</span></h3><p>K-fold cross validation is a resampling technique used to evaluate the performance of machine learning models. It works by splitting the dataset into k folds, and then training the model on k-1 folds and testing it on the remaining fold. This process is repeated k times, with each fold being used as the test set once. The results are typically averaged to obtain a more robust estimate of your model’s performance.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># k-fold cross validation implementation</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    </span><br><span class="line"><span class="comment"># split data into k-folds (train, test) datasets</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kfold_indices</span>(<span class="params">data, k</span>):</span><br><span class="line">	fold_size = <span class="built_in">len</span>(data) // k</span><br><span class="line">	indices = np.arange(<span class="built_in">len</span>(data))</span><br><span class="line">	flods = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">		test_indices = indices[i * fold_size, (i+<span class="number">1</span>)*fold_size]</span><br><span class="line">		train_indices = np.concatenate([indices[: i*fold_size], indices[(i+<span class="number">1</span>)*fold_size:]])</span><br><span class="line">		folds.append((train_indices, test_indices))</span><br><span class="line">	<span class="keyword">return</span> folds</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"><span class="comment"># perform k-fold validation</span></span><br><span class="line">fold_indices = kfold_indices(X, k)</span><br><span class="line">scores = []</span><br><span class="line"><span class="comment"># Iterate through each fold</span></span><br><span class="line"><span class="keyword">for</span> train_indices, test_indices <span class="keyword">in</span> fold_indices:</span><br><span class="line">    X_train, y_train = X[train_indices], y[train_indices]</span><br><span class="line">    X_test, y_test = X[test_indices], y[test_indices]</span><br><span class="line">    <span class="comment"># Train the model on the training data</span></span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    <span class="comment"># Make predictions on the test data</span></span><br><span class="line">    y_pred = model.predict(X_test)</span><br><span class="line">    <span class="comment"># Calculate the accuracy score for this fold</span></span><br><span class="line">    fold_score = accuracy_score(y_test, y_pred)</span><br><span class="line">    <span class="comment"># Append the fold score to the list of scores</span></span><br><span class="line">    scores.append(fold_score)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Calculate the mean accuracy across all folds</span></span><br><span class="line">mean_accuracy = np.mean(scores)</span><br></pre></td></tr></table></figure>

<h2><span id="deep-learning-algorithms">Deep Learning algorithms</span></h2><h3><span id="batch-normalization">Batch Normalization</span></h3><p>Why normalization?<br>The phenomenon by which the distributions of internal nodes (neurons) of a neural network change is referred to as <strong>Internal Covariate Shift</strong>. And we want to avoid it because it makes training the network slower, as the neurons are forced to re-adjust drastically their weights in one direction or another because of drastic changes in the outputs of the previous layers.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261845730.png" alt="norm"></p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212101170.png" alt="batch_norm"></p>
<ul>
<li>PyTorch implementation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchnorm_forward</span>(<span class="params">x, gamma, beta, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">    N, D = x.shape <span class="comment"># N is the batch size, D is the hidden size of each sample in batch</span></span><br><span class="line">    sample_mean = x.mean(axis=<span class="number">0</span>) <span class="comment"># shape [1, D]</span></span><br><span class="line">    sample_var = x.var(axis=<span class="number">0</span>) <span class="comment"># shape [1, D]</span></span><br><span class="line">    std = np.sqrt(sample_var + eps)</span><br><span class="line">    x_centered = x - sample_mean</span><br><span class="line">    x_norm = x_centered / std</span><br><span class="line">    out = gamma * x_norm + beta</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li>
<li>NumPy implementation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchnorm_forward</span>(<span class="params">x, scales,, bias, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">  mean = x.mean(axis=<span class="number">0</span>)  <span class="comment"># Shape = (w, h, c)</span></span><br><span class="line">  var  = x.var(axis=<span class="number">0</span>)</span><br><span class="line">  std = np.sqrt(var+ eps) <span class="comment"># shape = (w, h, c)</span></span><br><span class="line">  <span class="comment"># eps is used to avoid divisions by zero</span></span><br><span class="line">  <span class="comment"># Compute the normalized input</span></span><br><span class="line">  x_norm = (x - mean) / std <span class="comment"># shape (batch, w, h, c)</span></span><br><span class="line">  <span class="comment"># Output = scale * x_norm + bias</span></span><br><span class="line">  <span class="keyword">if</span> scales <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      x_norm *= scales  <span class="comment"># Multiplication for scales</span></span><br><span class="line">  <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      x_norm += bias <span class="comment"># Add bias</span></span><br><span class="line">  <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3><span id="layer-normalization">Layer Normalization</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212149406.png" alt="layer_norm"><br>Explanation:</p>
<blockquote>
<p>A well-known explanation of the success of LayerNorm is its re-centering and re-scaling invariance property. The former enables the model to be insensitive to shift noises on both inputs and weights, and the latter keeps the output representations intact when both inputs and weights are randomly scaled.</p>
</blockquote>
<ul>
<li><p>Numpy implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_norm</span>(<span class="params">x, g, b, eps: <span class="built_in">float</span> = <span class="number">1e-5</span></span>):</span><br><span class="line">    mean = np.mean(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    variance = np.var(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    x = (x - mean) / np.sqrt(variance + eps)  <span class="comment"># normalize x to have mean=0 and var=1 over last axis</span></span><br><span class="line">    <span class="keyword">return</span> g * x + b  <span class="comment"># scale and offset with gamma/beta params</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Pytorch implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># feature : (batch_size, seq_len, hidden_dim)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorma, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">farward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># (batch_size, seq_len, 1)</span></span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># (batch_size, seq_len, 1)</span></span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x-mean) / (std+self.eps) + self.b_2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3><span id="dropout">Dropout</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022024946.png" alt="dropout"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dropout</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self. drop_prob</span>):</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        self.dropout = (np.random.rand(*inputs.shape) &gt; self.dorp_prob)</span><br><span class="line">        <span class="keyword">return</span> np.where(self.dropout, inputs, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grads</span>):</span><br><span class="line">        <span class="keyword">return</span> np.where(self.dropout, grads, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3><span id="optimizers">Optimizers</span></h3><h4><span id="sgd-stochastic-gradient-descent">SGD (Stochastic Gradient Descent)</span></h4><p>Advantages:</p>
<ol>
<li>Frequent updates of model parameters, converges in less time</li>
<li>Requires less memory as no need to store values of loss functions</li>
<li>May get new minima’s<br>Disadvantages:</li>
<li>High variance in model parameters</li>
<li>May shoot even after achieving global minima</li>
<li>To get the same convergence as gradient descent needs to slowly reduce the value of learning rate</li>
</ol>
<h4><span id="mini-batch-gradient-descent-bgd">Mini-Batch Gradient Descent (BGD)</span></h4><p>Advantages:</p>
<ol>
<li>Frequently updates the model parameters and also has less variance</li>
<li>Requires medium amount of memory<br>All types of Gradient Descent have some challenges:</li>
<li>May get trapped at local minima</li>
<li>Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge</li>
<li>Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate</li>
</ol>
<h4><span id="momentum">Momentum</span></h4><p>Used in conjunction Stochastic Gradient Descent or Mini-Batch Gradient Descent, Momentum takes into account past gradients to smooth out the update.<br>This is seen in variable 𝑣, which is an exponentially weighted average of the gradient on previous steps. This results in minimizing oscillations and faster convergence.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071937092.png" alt="momentum"><br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071932599.png" alt="moment"><br>Compared with (minibatch) stochastic gradient descent the momentum method needs to maintain a set of auxiliary variables, i.e., velocity. It has the same shape as the gradients (and variables of the optimization problem). In the implementation below we call these variables states.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_momentum_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_momentum</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = hyperparams[<span class="string">&#x27;momentum&#x27;</span>] * v + p.grad</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="adagrad-adaptative-gradient">Adagrad (Adaptative Gradient)</span></h4><p>Adagard optimizer changes the learning rate for each parameter at every time step. Particularly, it tends to assign higher learning rates to infrequent features, which ensures that the parameter updates rely less on frequency and more on relevance.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071928626.png" alt="adagrad"><br>Advantages:</p>
<ol>
<li>Learning rate changes for each training parameters</li>
<li>Don’t need to manually tune the learning rate</li>
<li>Able to train on sparse data<br>Disadvantages:</li>
<li>Computationally expensive as a need to calculate the second order derivative</li>
<li>The learning rate is always decreasing results in slow training</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adagrad_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adagrad</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            s[:] += torch.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / torch.sqrt(s + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="adadelta">Adadelta</span></h4><p>Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071955958.png" alt="adadelta"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adadelta_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w, s_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    delta_w, delta_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adadelta</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    rho, eps = hyperparams[<span class="string">&#x27;rho&#x27;</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># In-place updates via [:]</span></span><br><span class="line">            s[:] = rho * s + (<span class="number">1</span> - rho) * torch.square(p.grad)</span><br><span class="line">            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad</span><br><span class="line">            p[:] -= g</span><br><span class="line">            delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="rmsprop-root-mean-square-propagation">RMSProp (Root Mean Square Propagation)</span></h4><p>RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale coefficients. RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique to adjust the coefficient-wise preconditioner. The learning rate needs to be scheduled by the experimenter in practice.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071939471.png" alt="RMSProp"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rmsprop_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsprop</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    gamma, eps = hyperparams[<span class="string">&#x27;gamma&#x27;</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            s[:] = gamma * s + (<span class="number">1</span> - gamma) * torch.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / torch.sqrt(s + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)</span></h4><p>Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. Adam computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072005801.png" alt="adam"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w, v_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    s_w, s_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad</span><br><span class="line">            s[:] = beta2 * s + (<span class="number">1</span> - beta2) * torch.square(p.grad)</span><br><span class="line">            v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr)</span><br><span class="line">                                                       + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>Note: <strong>AdamW</strong>(Adam with decoupled weight decay)</p>
<h2><span id="cv-algorithms">CV algorithms</span></h2><h3><span id="cnn">CNN</span></h3><p>Components of CNN: Convolution, Pooling, Flatten, Fully connected layer<br>Basic PyTorch CNN implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):</span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		self.layer = nn.Sequential( <span class="comment"># (N, 32, 32, 1) N:batch_size, 32: height, 32: width, 1: channel</span></span><br><span class="line">			nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>), <span class="comment"># 1: in_channels, 6: out_channels, Convolution -&gt; (N, 28, 28, 6)</span></span><br><span class="line">			nn.BatchNorm2d(<span class="number">6</span>), <span class="comment"># batch normalization (N, 28, 28, 6)</span></span><br><span class="line">			nn.ReLU(),</span><br><span class="line">			nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>) <span class="comment"># Max Pooling -&gt; (N, 14, 14, 6)</span></span><br><span class="line">			)</span><br><span class="line">        self.fc = nn.linear(<span class="number">14</span>*<span class="number">14</span>*<span class="number">6</span>, num_classes)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.layer(x)</span><br><span class="line">        out	= out.reshape(out.size(<span class="number">0</span>), -<span class="number">1</span>) <span class="comment"># Flatten -&gt; (N, 14, 14, 6) =&gt; (N, 14*14*6)</span></span><br><span class="line">        out = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>PyTorch implementation for Convolution and Pooling:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">image, kernel, padding_size, stride</span>):  <span class="comment">#cross-correlation operation</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute 2D cross-correlation.&quot;&quot;&quot;</span></span><br><span class="line">    h, w = kernel.shape</span><br><span class="line">    image = np.pad(image, padding_size) </span><br><span class="line">    Y = torch.zeros(((image.shape[<span class="number">0</span>] - h) // stride + <span class="number">1</span>, (image.shape[<span class="number">1</span>] - w) // stride + <span class="number">1</span>))</span><br><span class="line">	start_pos_h, start_pos_w = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[start_pos_h : start_pos_h + h, start_pos_w : start_pos_w + w] * kernel).<span class="built_in">sum</span>()</span><br><span class="line">            start_pos_w += stride</span><br><span class="line">        start_pos_h += stride</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&quot;max&quot;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&quot;max&quot;</span>:</span><br><span class="line">                Y[i,j] = X[i:i+p_h, j:j+p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> model == <span class="string">&quot;avg&quot;</span>:</span><br><span class="line">                Y[i,j] = X[i:i+p_h, j:j+p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p>NumPy implementation of Convolution and Pooling</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">image, num_filters, stride, kernel_size, padding=<span class="number">0</span></span>):</span><br><span class="line">    <span class="comment"># init filters</span></span><br><span class="line">    filters = np.random.randn(num_filters, kernel_size, kernel_size)*<span class="number">0.1</span></span><br><span class="line">    <span class="comment"># image size (num_channels, 28, 28)</span></span><br><span class="line">    in_dim = image.shape[<span class="number">1</span>] <span class="comment"># 28</span></span><br><span class="line">    out_dim = (in_dim - kernel_size) // stride + <span class="number">1</span> <span class="comment"># without padding</span></span><br><span class="line">    out = np.zeros((num_filters, out_dim, out_dim)) <span class="comment"># the Convolution output</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(num_filters):</span><br><span class="line">        temp_y = out_y = <span class="number">0</span> </span><br><span class="line">        <span class="keyword">while</span> temp_y + kernel_size &lt;= in_dim:</span><br><span class="line">            temp_x = out_x = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> temp_x + kernel_size &lt;= in_dim:</span><br><span class="line">                patch = image[:, temp_x:temp_x+kernel_size, temp_y:temp_y+kernel_size]</span><br><span class="line">                out[f, out_y, out_x] = np.<span class="built_in">sum</span>(filters[f] * patch)</span><br><span class="line">                temp_x += stride</span><br><span class="line">                out_x += <span class="number">1</span></span><br><span class="line">            temp_y += stride</span><br><span class="line">            out_y += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">max_pooling</span>(<span class="params">image, stride, pooling_size</span>):</span><br><span class="line">    num_channels, height, width = image.shape</span><br><span class="line">    out_h = (height - pooling_size) // stride + <span class="number">1</span></span><br><span class="line">    out_w = (width - pooling_size) // stride + <span class="number">1</span></span><br><span class="line">    out = np.zeros(num_channels, out_h, out_w)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_channels):</span><br><span class="line">        temp_y = out_y = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> temp_y + pooling_size &lt;= height:</span><br><span class="line">            temp_x = out_x = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> temp_x + pooling_size &lt;= width:</span><br><span class="line">                patch = image[i, temp_y:temp_y+pooling_size, temp_x:temp_x+pooling_size]</span><br><span class="line">                out[i, out_y, out_x] = np.<span class="built_in">max</span>(patch)</span><br><span class="line">                temp_x += stride</span><br><span class="line">                out_x += <span class="number">1</span></span><br><span class="line">            temp_y += stride</span><br><span class="line">            out_y += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h3><span id="patch-embedding">Patch Embedding</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2D image to patch embedding</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size=<span class="number">224</span>, path_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span>, flatten=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_size = (image_size, image_size)</span><br><span class="line">        path_size = (path_size, path_size)</span><br><span class="line">        self.image_size = image_size</span><br><span class="line">        self.path_size = path_size</span><br><span class="line">        self.grid_size = (image_size[<span class="number">0</span>]//patch_size[<span class="number">0</span>], image_size[<span class="number">1</span>]//path_size[<span class="number">1</span>])</span><br><span class="line">        self.num_patches = self.grid_size[<span class="number">0</span>] * self.grid_size[<span class="number">1</span>]</span><br><span class="line">        self.flatten = flatten</span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Output shape: (batch size, no. of patches, hidden_dim)</span></span><br><span class="line">        B, C, H, W = x.shape <span class="comment"># (bs, 3, 224, 224)</span></span><br><span class="line">        x = self.proj(x) <span class="comment"># (bs, 768, 14, 14)</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>) <span class="comment"># (bs, 768, 196)</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (bs, 196, 768)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2><span id="nlp-algorithms">NLP algorithms</span></h2><h3><span id="basic-preprocessing">Basic Preprocessing</span></h3><p>Here are the <a target="_blank" rel="noopener" href="https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing">steps</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># remove URLs</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_urls</span>(<span class="params">text</span>):</span><br><span class="line">    url_pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;https?://\S+|www\.\S+&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> url_pattern.sub(<span class="string">r&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove html tags</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_html</span>(<span class="params">text</span>):</span><br><span class="line">    html_pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;.*?&gt;&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> html_pattern.sub(<span class="string">r&#x27;&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize: Splitting the sentence into words</span></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line">sentence = <span class="string">&quot;Books are on the table&quot;</span></span><br><span class="line">words = word_tokenize(sentence)</span><br><span class="line"></span><br><span class="line"><span class="comment"># lower case</span></span><br><span class="line">word = word.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove whitespace head and tail</span></span><br><span class="line">word = word.strip(<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove Punctuation</span></span><br><span class="line"><span class="keyword">import</span> string  </span><br><span class="line">puncts = string.punctuation</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove stop words</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="built_in">set</span>(stopwords.words(<span class="string">&#x27;english&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove emoji</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">remove_emoji</span>(<span class="params">string</span>):</span><br><span class="line">    emoji_pattern = re.<span class="built_in">compile</span>(<span class="string">&quot;[&quot;</span></span><br><span class="line">                           <span class="string">u&quot;\U0001F600-\U0001F64F&quot;</span>  <span class="comment"># emoticons</span></span><br><span class="line">                           <span class="string">u&quot;\U0001F300-\U0001F5FF&quot;</span>  <span class="comment"># symbols &amp; pictographs</span></span><br><span class="line">                           <span class="string">u&quot;\U0001F680-\U0001F6FF&quot;</span>  <span class="comment"># transport &amp; map symbols</span></span><br><span class="line">                           <span class="string">u&quot;\U0001F1E0-\U0001F1FF&quot;</span>  <span class="comment"># flags (iOS)</span></span><br><span class="line">                           <span class="string">u&quot;\U00002702-\U000027B0&quot;</span></span><br><span class="line">                           <span class="string">u&quot;\U000024C2-\U0001F251&quot;</span></span><br><span class="line">                           <span class="string">&quot;]+&quot;</span>, flags=re.UNICODE)</span><br><span class="line">    <span class="keyword">return</span> emoji_pattern.sub(<span class="string">r&#x27;&#x27;</span>, string)</span><br><span class="line"></span><br><span class="line"><span class="comment"># stemming or Lemmatization</span></span><br><span class="line"><span class="comment"># A better efficient way to proceed is to first lemmatise and then stem, but stemming alone is also fine for few problems statements, here we will not lemmatise.</span></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">stemmer.stem(word) </span><br></pre></td></tr></table></figure>

<h3><span id="cosine-similarity">Cosine similarity</span></h3><p>NumPy Implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))</span><br></pre></td></tr></table></figure>
<p>python implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dot_product</span>(<span class="params">A,B</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>([a*b <span class="keyword">for</span> a,b <span class="keyword">in</span> <span class="built_in">zip</span>(A,B)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">A,B</span>):</span><br><span class="line">    <span class="keyword">return</span> dot_product(A,B) / (dot_product(A,A)**<span class="number">0.5</span> * dot_product(B,B)**<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<h3><span id="tf-idf">TF-IDF</span></h3><p>Terminology:<br>TF - Term Frequency, DF - Document Frequency, IDF - Inverse Document Frequency.<br><em>t</em> — term (word), <em>d</em> — document (set of words), <em>N</em> — count of corpus, corpus — the total document set.<br>Formula:<br>TF(<em>t,d</em>) = count of <em>t</em> in <em>d</em> / number of words in <em>d</em><br>DF(<em>t</em>) =  the number of documents containing the term <em>t</em><br>IDF(<em>t</em>) = <em>N</em> / DF(<em>t</em>)<br>To avoid IDF value explodes we take the log of IDF ==&gt; IDF(<em>t</em>) = log( <em>N</em> / (DF(<em>t</em>) + 1) )<br>TD-IDF(<em>t, d</em>) = TF(<em>t,d</em>) * IDF(<em>t</em>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize </span><br><span class="line"> </span><br><span class="line">text = [<span class="string">&#x27;Topic sentences are similar to mini thesis statements.\</span></span><br><span class="line"><span class="string">        Like a thesis statement, a topic sentence has a specific \</span></span><br><span class="line"><span class="string">        main point. Whereas the thesis is the main point of the essay&#x27;</span>,\</span><br><span class="line">        <span class="string">&#x27;the topic sentence is the main point of the paragraph.\</span></span><br><span class="line"><span class="string">        Like the thesis statement, a topic sentence has a unifying function. \</span></span><br><span class="line"><span class="string">        But a thesis statement or topic sentence alone doesn’t guarantee unity.&#x27;</span>, \</span><br><span class="line">        <span class="string">&#x27;An essay is unified if all the paragraphs relate to the thesis,\</span></span><br><span class="line"><span class="string">        whereas a paragraph is unified if all the sentences relate to the topic sentence.&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#preprocessing the text data</span></span><br><span class="line">sentences = []</span><br><span class="line">word_list = []</span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> text:</span><br><span class="line">    x = [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(sent) <span class="keyword">if</span> w.isalpha()]</span><br><span class="line">    sentences.append(x)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> x:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_list:</span><br><span class="line">            word_list.append(word)</span><br><span class="line"></span><br><span class="line"><span class="comment"># total number of docs in corpus</span></span><br><span class="line">total_doc = <span class="built_in">len</span>(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate term frequency for all documents</span></span><br><span class="line">term_freq_docs = [] <span class="comment"># shape: (num_docs, num_unique_words)</span></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> sentences:</span><br><span class="line">    term_freq = [<span class="number">0</span>]*<span class="built_in">len</span>(word_list)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">        idx = word_list.index(word)</span><br><span class="line">        term_freq[idx] += <span class="number">1</span></span><br><span class="line">    term_freq = [x/<span class="built_in">len</span>(sent) <span class="keyword">for</span> x <span class="keyword">in</span> term_freq]</span><br><span class="line">    term_freq_docs.append(term_freq)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate inverse document frequency</span></span><br><span class="line">inverse_doc_freq = [<span class="number">0</span>]*<span class="built_in">len</span>(word_list) <span class="comment"># shape: num_unique_words</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> word_list:</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> sent:</span><br><span class="line">            idx = word_list.index(w)</span><br><span class="line">            inverse_doc_freq[idx] += <span class="number">1</span></span><br><span class="line">    inverse_doc_freq = [np.log(total_doc / (x + <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> inverse_doc_freq]</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate tf-idf </span></span><br><span class="line">tf_idf_all = [] <span class="comment"># shape: (num_docs, num_unique_words)</span></span><br><span class="line"><span class="keyword">for</span> term_freq <span class="keyword">in</span> term_freq_docs:</span><br><span class="line">    tf_idf = []</span><br><span class="line">    <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(term_freq):</span><br><span class="line">        tf_idf.append(term_freq[idx] * inverse_doc_freq[idx])</span><br><span class="line">    tf_idf_all.append(tf_idf)  </span><br></pre></td></tr></table></figure>
<p>Sometimes we use TF-IDF for query-document retrieval task, in this case, we will calculate TF and IDF for each word in query according to different documents. Here is the example: <a target="_blank" rel="noopener" href="https://medium.com/nlplanet/two-minutes-nlp-learn-tf-idf-with-easy-examples-7c15957b4cb3">blog</a></p>
<h3><span id="fasttext">FastText</span></h3><p>fastText is usually used to do classification tasks, it has two main improvements:</p>
<ul>
<li>used char-level n-gram as additional word embedding features</li>
<li>used hierarchical softmax to reduce computation cost<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FastText</span>(nn.module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>(FastText, self).__init__()</span><br><span class="line">        self.embedding_word = nn.Embedding(</span><br><span class="line">            config.vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.vocab_size-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.embedding_bigram = nn.Embedding(</span><br><span class="line">            config.ngram_vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.ngram_vocab_size-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.embedding_trigram = nn.Embedding(</span><br><span class="line">            config.ngram_vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.ngram_vocab_size-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout_rate)</span><br><span class="line">        self.fc1 = nn.Linear(config.embed_size * <span class="number">3</span>, config.hidden_size)</span><br><span class="line">        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param x: x[0] (words), x[1] (bigram), x[2] (trigram): all have shape [batch_size, seq_length] </span></span><br><span class="line"><span class="string">        :return: [batch_size, num_classes] classification task</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        embed_bow = self.embedding_word(x[<span class="number">0</span>])</span><br><span class="line">        embed_bigram = self.embedding_bigram(x[<span class="number">1</span>])</span><br><span class="line">        embed_trigram = self.embedding_trigram(x[<span class="number">2</span>])</span><br><span class="line">        <span class="built_in">input</span> = torch.cat((embed_bow, embed_bigram, embed_trigram), -<span class="number">1</span>)</span><br><span class="line">        out = <span class="built_in">input</span>.mean(dim=<span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3><span id="rnn-implementation">RNN implementation</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_hiddens, sigma=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_inputs = num_inputs <span class="comment"># dimension size of each step&#x27;s input </span></span><br><span class="line">        self.num_hiddens = self.num_hiddens</span><br><span class="line">        self.W_xh = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)</span><br><span class="line">        self.W_hh = nn.Parameter(torch.randn(num_hiddens, num_hiddens) * sigma)</span><br><span class="line">        self.b_h = nn.Parameter(torch.randn(num_hiddens))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size, seq_len = inputs.shape</span><br><span class="line">        <span class="keyword">if</span> state == <span class="literal">None</span>:</span><br><span class="line">            state = torch.zeros((batch_size, self.num_hiddens)) <span class="comment"># the state of last step</span></span><br><span class="line">        outputs = [] <span class="comment"># shape: (num_steps, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">for</span> x_step <span class="keyword">in</span> inputs: <span class="comment"># shape of inputs: (num_steps, batch_size, num_inputs)</span></span><br><span class="line">            <span class="comment"># in NLP the num_steps equals seq_len, num_inputs equals token embedding size</span></span><br><span class="line">            state = torch.tahn(torch.matmul(x_step, W_xh) + torch.matmul(state, self.W_hh) + self.b_h) <span class="comment"># (batch_size, num_hiddens)</span></span><br><span class="line">            outputs.append(state)</span><br><span class="line">        <span class="keyword">return</span> outputs, state</span><br></pre></td></tr></table></figure>

<h2><span id="llm-algorithms">LLM algorithms</span></h2><h3><span id="lora-low-rank-adaption-implementation">LoRA (Low Rank Adaption) Implementation</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031604097.png" alt="lora"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearLoRA</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    A low rank adapted linear layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_dim: An integer representing the input dimension of the linear layers</span></span><br><span class="line"><span class="string">        out_dim: An integer representing the output dimension of the linear layer</span></span><br><span class="line"><span class="string">        r: An integer representing the rank of low-rank approximated matrices</span></span><br><span class="line"><span class="string">        lora_alpha: An integer representing the number of scaling constant alpha / r</span></span><br><span class="line"><span class="string">        lora_dropout: A float between 0 and 1 representing the dropout probability</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim:<span class="built_in">int</span>, out_dim:<span class="built_in">int</span>, r: <span class="built_in">int</span>=<span class="number">8</span>, lora_alpha: <span class="built_in">int</span>=<span class="number">16</span>, lora_dropout: <span class="built_in">float</span>=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.r = r <span class="comment"># must &gt;= 1</span></span><br><span class="line">        self.lora_alpha = lora_alpha</span><br><span class="line">        self.lora_dropout = nn.Dropout(lora_dropout)</span><br><span class="line">        <span class="comment"># recreate the linear layer and freeze it (the actual weight values will be copied in outside of this class)</span></span><br><span class="line">        self.pretrained = nn.Linear(in_dim, out_dim, bias=<span class="literal">True</span>)</span><br><span class="line">        self.pretrained.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># create the low-rank A matrix and initialize with same method as in Hugging face PEFT</span></span><br><span class="line">        self.lora_A = nn.Linear(in_dim, r, bias=<span class="literal">True</span>)</span><br><span class="line">        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(<span class="number">5</span>)) <span class="comment"># or nn.init.normal_(self.lora_A, mean=0, std=1)</span></span><br><span class="line">        <span class="comment"># another method to define LoRA_A matrix</span></span><br><span class="line">        <span class="comment"># self.A = nn.Parameter(torch.randn(in_dim, r) * math.sqrt(r))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># create the low rank B matrix and initialize to zero</span></span><br><span class="line">        self.lora_B = nn.Linear(r, out_dim, bias=<span class="literal">True</span>)</span><br><span class="line">        nn.init.constant_(self.lora_B.weight, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># another method to define LoRA_B matrix</span></span><br><span class="line">        <span class="comment"># self.B = nn.Parameter(torch.zeros(r, out_dim))</span></span><br><span class="line">        self.scaling = self.lora_alpha / self.r</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        pretrained_out = self.pretrained(x)</span><br><span class="line">        lora_out = self.lora_A(x)</span><br><span class="line">        lora_out = self.lora_B(lora_out)</span><br><span class="line">        <span class="comment"># if we used another method defined A and B:</span></span><br><span class="line">        <span class="comment"># lora_out = x @ self.A @ self.B</span></span><br><span class="line">        lora_out = self.lora_dropout(lora_out)</span><br><span class="line">        lora_out = lora_out * self.scaling</span><br><span class="line">        <span class="keyword">return</span> pretrained_out + lora_out</span><br><span class="line"></span><br><span class="line"><span class="comment">## Another simpale to implement LoRA</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoRALayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, out_dim, rank, alpha</span>):</span><br><span class="line">        std_dev = <span class="number">1</span> / torch.sqrt(torch.tensor(rank).<span class="built_in">float</span>())</span><br><span class="line">        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)</span><br><span class="line">        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.alpha * (x @ self.A @ self.B)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h3><span id="autoregressive-decoding">Autoregressive decoding</span></h3><p>Decoder models generate tokens one by one during inference, e.g. GPT, Llama, BART, T5 etc. We can this autoregressive decoding and it is the main bottleneck of inference speed for decoder models.<br>Here is an sample implementation of this autoregressive decoding process:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">model, idx, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Take a conditioning sequence of indices idx (Tensor: batch_size, input_length) abd complete the sequence max_new_tokens times,</span></span><br><span class="line"><span class="string">    feeding the predictions back into the model each time.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    mode.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        <span class="comment"># forward the model to get the logits for the index in the sequence</span></span><br><span class="line">        logits, _ = mode(idx) <span class="comment"># logits: (batch_size, input_length, vocab_size)</span></span><br><span class="line">        <span class="comment"># get the logits at the final step and scale by desired temperature</span></span><br><span class="line">        <span class="comment"># While applying temperature can make a distribution less random, in its limit, when setting temperature -&gt; 0, temperature scaled sampling becomes equal to greedy decoding</span></span><br><span class="line">        logits = logits[:,-<span class="number">1</span>,:] / temperature</span><br><span class="line">        <span class="comment"># optionally crop the logits to only the top k options</span></span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            v, _ = torch.topk(logits, <span class="built_in">min</span>(top_k, logits.size(-<span class="number">1</span>)))</span><br><span class="line">            logits[logits &lt; v[:, [-<span class="number">1</span>]]] = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">        <span class="comment"># apply softmax to convert logits to (normalized) probabilities</span></span><br><span class="line">        probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># sample from the distribution</span></span><br><span class="line">        idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># append sampled index to the running sequence to continue</span></span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>

<h3><span id="speculative-decoding">Speculative Decoding</span></h3><p>This technology is used in LLM inference stage to speed up Auto-regressive decoding steps.<br>Main idea: (<a target="_blank" rel="noopener" href="https://jaykmody.com/blog/speculative-sampling/">source</a>)</p>
<blockquote>
<p>In speculative sampling, we have two models:</p>
<ol>
<li>A smaller, faster draft model</li>
<li>A larger, slower target model<br>The idea is that the draft model speculates what the output is steps into the future, while the target model determines how many of those tokens we should accept.</li>
</ol>
</blockquote>
<p>The intuition behind speculative sampling is that <em>certain strings of tokens (common phrases, pronouns, punctuation, etc …) are fairly easy to predict, so a smaller, less powerful, but faster draft model should be able to quickly predict these instead of having our slower target model doing all the work</em>.</p>
<p>The algorithm is following:</p>
<ol>
<li>The draft model decodes <em>K</em> tokens in the regular autoregressive fashion.</li>
<li>We get the probability outputs of the target and draft model on the new predicted sequence.</li>
<li>We compare the target and draft model probabilities to determine how many of the <em>K</em> tokens we want to keep based on some rejection criteria. If a token is rejected, we resample it using a combination of the two distributions and don’t accept any more tokens.</li>
<li>If all <em>K</em> tokens are accepted, we can sample an additional final token from the target model probability output.</li>
</ol>
<p>As such, instead of decoding a single token at each iteration, speculative sampling decodes between 1 to <em>K+1</em> tokens per iteration. If no tokens are accepted, we resample guaranteeing at least 1 token is decoded. If all <em>K</em> tokens are accepted, then we can also sample a final token from the target models probability distribution, giving us a total of <em>K+1</em> tokens decoded.</p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031620531.png" alt="sepcu_decoding"></p>
<h2><span id="mathematics">Mathematics</span></h2><h3><span id="different-distributions">Different distributions</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401312314722.png" alt="dist"></p>
<h2><span id="references">References</span></h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wevolf/p/15195143.html">https://www.cnblogs.com/wevolf/p/15195143.html</a></li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">https://lilianweng.github.io/posts/2021-05-31-contrastive/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hkproj/pytorch-llama/tree/main">https://github.com/hkproj/pytorch-llama/tree/main</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.07467.pdf">https://arxiv.org/pdf/1910.07467.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6">https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6</a></li>
<li><a target="_blank" rel="noopener" href="https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum">https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.15596.pdf">https://arxiv.org/pdf/2211.15596.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://spotintelligence.com/2023/09/14/mean-average-precision/">https://spotintelligence.com/2023/09/14/mean-average-precision/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/ajinkyaabhang/implementing-tpr-fpr-roc-auc-from-scratch">https://www.kaggle.com/code/ajinkyaabhang/implementing-tpr-fpr-roc-auc-from-scratch</a></li>
<li><a target="_blank" rel="noopener" href="https://ethen8181.github.io/machine-learning/model_selection/auc/auc.html">https://ethen8181.github.io/machine-learning/model_selection/auc/auc.html</a></li>
</ul>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/ichbinhandsome">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">GitHub</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/besser-46">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.linkedin.com/in/ruixiang-w-85769a199">
            <span class="icon">
              <i class="fab fa-linkedin"></i>
            </span>

            <span class="label">LinkedIn</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/17/job-summary/" rel="prev" title="2021秋招回顾">
      <i class="fa fa-chevron-left"></i> 2021秋招回顾
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/10/22/Attention-Implementaion/" rel="next" title="Multi-Head Self Attention Implementation">
      Multi-Head Self Attention Implementation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">Classic Machine Learning algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">K-means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text">KNN implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.</span> <span class="nav-text">Logistic regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.4.</span> <span class="nav-text">Linear Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.5.</span> <span class="nav-text">Activation Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.6.</span> <span class="nav-text">Loss Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.7.</span> <span class="nav-text">Evaluation metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.1.</span> <span class="nav-text">Recall, Precision, F1, accuracy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.2.</span> <span class="nav-text">Marco &amp; Micro average recall&#x2F;precision</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.3.</span> <span class="nav-text">ROC and AUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.4.</span> <span class="nav-text">BLEU score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.5.</span> <span class="nav-text">Rouge score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.6.</span> <span class="nav-text">Perplexity</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.7.</span> <span class="nav-text">MRR, MAP, NDCG</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.8.</span> <span class="nav-text">Cross Validation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">Deep Learning algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.2.</span> <span class="nav-text">Layer Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.3.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.4.</span> <span class="nav-text">Optimizers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.1.</span> <span class="nav-text">SGD (Stochastic Gradient Descent)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.2.</span> <span class="nav-text">Mini-Batch Gradient Descent (BGD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.3.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.4.</span> <span class="nav-text">Adagrad (Adaptative Gradient)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.5.</span> <span class="nav-text">Adadelta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.6.</span> <span class="nav-text">RMSProp (Root Mean Square Propagation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.7.</span> <span class="nav-text">Adam (Adaptive Moment Estimation)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">CV algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.</span> <span class="nav-text">CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.2.</span> <span class="nav-text">Patch Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">NLP algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.1.</span> <span class="nav-text">Basic Preprocessing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.2.</span> <span class="nav-text">Cosine similarity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.3.</span> <span class="nav-text">TF-IDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.4.</span> <span class="nav-text">FastText</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.5.</span> <span class="nav-text">RNN implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">LLM algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.1.</span> <span class="nav-text">LoRA (Low Rank Adaption) Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.2.</span> <span class="nav-text">Autoregressive decoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.3.</span> <span class="nav-text">Speculative Decoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">6.</span> <span class="nav-text">Mathematics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">6.1.</span> <span class="nav-text">Different distributions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruixiang Wang</p>
  <div class="site-description" itemprop="description">somthing interesting</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ichbinhandsome" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ichbinhandsome" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/besser-46" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;besser-46" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/ruixiang-w-85769a199" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;ruixiang-w-85769a199" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.kaggle.com/ruixiangwang" title="Kaggle → https:&#x2F;&#x2F;www.kaggle.com&#x2F;ruixiangwang" rel="noopener" target="_blank"><i class="fab fa-kaggle fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fas fa-biohazard"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruixiang Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bgBMKRcFSCM47e8KkwhkNhxA-MdYXbMMI","app_key":"gyzjWP17vkNtWyHEjWdY5c8Y","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":250,"height":500},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
