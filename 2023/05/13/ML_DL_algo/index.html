<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ichbinhandsome.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="This blog aims to implement some important Machine Learning, Deep Learning, Natural Language Processing, Large Language Models related algorithms from scratch, which can help people get deeper underst">
<meta property="og:type" content="article">
<meta property="og:title" content="Implementation of ML &amp; DL &amp; NLP algorithms (WIP)">
<meta property="og:url" content="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/index.html">
<meta property="og:site_name" content="Ruixiang&#39;s blog">
<meta property="og:description" content="This blog aims to implement some important Machine Learning, Deep Learning, Natural Language Processing, Large Language Models related algorithms from scratch, which can help people get deeper underst">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022009002.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402040009042.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261757405.jpeg">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072019388.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072025852.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072027841.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041721113.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041730706.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041813997.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041816809.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822903.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822446.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041843225.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041856323.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041959054.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042007454.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042051887.png">
<meta property="og:image" content="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/!%5B%5D(https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041950356.png)">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261845730.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212101170.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212149406.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022024946.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071937092.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071932599.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071928626.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071955958.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071939471.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072005801.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031604097.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031620531.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401312314722.png">
<meta property="article:published_time" content="2023-05-13T16:53:38.000Z">
<meta property="article:modified_time" content="2024-03-12T21:19:05.683Z">
<meta property="article:author" content="Ruixiang Wang">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png">

<link rel="canonical" href="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Implementation of ML & DL & NLP algorithms (WIP) | Ruixiang's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ruixiang's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">work harder, study better, do faster, become stronger</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ichbinhandsome.github.io/2023/05/13/ML_DL_algo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruixiang Wang">
      <meta itemprop="description" content="somthing interesting">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruixiang's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Implementation of ML & DL & NLP algorithms (WIP)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-05-13 18:53:38" itemprop="dateCreated datePublished" datetime="2023-05-13T18:53:38+02:00">2023-05-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-03-12 22:19:05" itemprop="dateModified" datetime="2024-03-12T22:19:05+01:00">2024-03-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          
            <span id="/2023/05/13/ML_DL_algo/" class="post-meta-item leancloud_visitors" data-flag-title="Implementation of ML & DL & NLP algorithms (WIP)" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>This blog aims to implement some important Machine Learning, Deep Learning, Natural Language Processing, Large Language Models related algorithms from scratch, which can help people get deeper understanding about how these algorithms work in real code. </p>
<ul>
<li><a href="#classic-machine-learning-algorithms">Classic Machine Learning algorithms</a><ul>
<li><a href="#k-means">K-means</a></li>
<li><a href="#logistic-regression">Logistic regression</a></li>
<li><a href="#linear-regression">Linear Regression</a></li>
<li><a href="#activation-functions">Activation Functions</a></li>
<li><a href="#loss-functions">Loss Functions</a></li>
<li><a href="#evaluation-metrics">Evaluation metrics</a><ul>
<li><a href="#recall-precision-f1-accuracy">Recall, Precision, F1, accuracy</a></li>
<li><a href="#marco--micro-average-recallprecision">Marco &amp; Micro average recall/precision</a></li>
<li><a href="#roc-and-auc">ROC and AUC</a></li>
<li><a href="#bleu-score">BLEU score</a></li>
<li><a href="#rouge-score">Rouge score</a></li>
<li><a href="#perplexity">Perplexity</a></li>
<li><a href="#mrr-map-ndcg">MRR, MAP, NDCG</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#deep-learning-algorithms">Deep Learning algorithms</a><ul>
<li><a href="#batch-normalization">Batch Normalization</a></li>
<li><a href="#layer-normalization">Layer Normalization</a></li>
<li><a href="#dropout">Dropout</a></li>
<li><a href="#optimizers">Optimizers</a><ul>
<li><a href="#sgd-stochastic-gradient-descent">SGD (Stochastic Gradient Descent)</a></li>
<li><a href="#mini-batch-gradient-descent-bgd">Mini-Batch Gradient Descent (BGD)</a></li>
<li><a href="#momentum">Momentum</a></li>
<li><a href="#adagrad-adaptative-gradient">Adagrad (Adaptative Gradient)</a></li>
<li><a href="#adadelta">Adadelta</a></li>
<li><a href="#rmsprop-root-mean-square-propagation">RMSProp (Root Mean Square Propagation)</a></li>
<li><a href="#adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#nlp-algorithms">NLP algorithms</a><ul>
<li><a href="#fasttext">FastText</a></li>
</ul>
</li>
<li><a href="#llm-algorithms">LLM algorithms</a><ul>
<li><a href="#lora-low-rank-adaption-implementation">LoRA (Low Rank Adaption) Implementation</a></li>
<li><a href="#speculative-decoding">Speculative Decoding</a></li>
</ul>
</li>
<li><a href="#mathematics">Mathematics</a><ul>
<li><a href="#different-distributions">Different distributions</a></li>
</ul>
</li>
<li><a href="#references">References</a><span id="more"></span>

</li>
</ul>
<h2><span id="classic-machine-learning-algorithms">Classic Machine Learning algorithms</span></h2><h3><span id="k-means">K-means</span></h3><ul>
<li><p>Steps:</p>
<ol>
<li>Clusters the data into <em>k</em> groups where <em>k</em> is predefined.</li>
<li>Select <em>k</em> points at <em>random</em> as cluster centers.</li>
<li>Assign objects to their closest cluster center according to the <em>Euclidean distance</em> function.</li>
<li>Calculate the <em>centroid</em> or <em>mean</em> of all objects in each cluster.</li>
<li>Repeat steps 2, 3 and 4 until the same points are assigned to each cluster in consecutive rounds. </li>
</ol>
</li>
<li><p>Implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_dist</span>(<span class="params">vector1, vector2</span>):</span><br><span class="line">    <span class="comment"># euclidian distance </span></span><br><span class="line">    eucl_dist = np.sqrt(np.<span class="built_in">sum</span>((vector1 - vector2)**<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> eucl_dist</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">k_means</span>(<span class="params">data, k</span>):</span><br><span class="line">    n_samples, n_shape = data.shape</span><br><span class="line">    <span class="comment"># random initialize centroids </span></span><br><span class="line">    centroid_index = np.random.choice(<span class="built_in">range</span>(n_samples), k)</span><br><span class="line">    centroid_coords = data[centroid_index]</span><br><span class="line">    <span class="comment"># label of each data point</span></span><br><span class="line">    labels = [-<span class="number">1</span>] * n_samples</span><br><span class="line">    <span class="comment"># iterate until centroid no longer change</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">            min_dist = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">            min_cluster_idx = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                dist = cal_dist(data[i], centroid_coords[j])</span><br><span class="line">                <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">                    min_dist = dist</span><br><span class="line">                    min_cluster_idx = k</span><br><span class="line">            <span class="comment"># update cluster index for each point</span></span><br><span class="line">            label[i] = min_cluster_idx</span><br><span class="line">        <span class="comment"># flag indicate if the centroid change of not: stop condition for while loop</span></span><br><span class="line">        centroid_update = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># recalculate the centroids </span></span><br><span class="line">        <span class="keyword">for</span> i, centroid <span class="keyword">in</span> <span class="built_in">enumerate</span>(centroid_coords):</span><br><span class="line">            <span class="comment"># gather all data points for each cluster</span></span><br><span class="line">            cluster_points = []</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">                <span class="keyword">if</span> label[k] == i:</span><br><span class="line">                    cluster_points.append(data[k])</span><br><span class="line">            cluster_points = np.concatenate([cluster_points], axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># calculate new centroid</span></span><br><span class="line">            new_centroid_coord = np.mean(cluster_points, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># see if the centroid change for each cluster</span></span><br><span class="line">            <span class="keyword">if</span> cal_dist(centroid, new_centroid_coord) &gt; <span class="number">1e-5</span>:</span><br><span class="line">                centroid_coords[i] = new_centroid_coord</span><br><span class="line">                centroid_update = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> centroid_update:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> labels</span><br><span class="line">                        </span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3><span id="logistic-regression">Logistic regression</span></h3><p>NumPy implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://blog.51cto.com/u_15661962/5531479 </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainable weights and bias init</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weight_init</span>(<span class="params">n_features</span>):</span><br><span class="line">    <span class="comment"># n_features: size of each data point</span></span><br><span class="line">    w = np.zeros(<span class="number">1</span>, n_features)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"><span class="comment"># activation function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function: binary cross entropy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bce_loss</span>(<span class="params">pred, y, n</span>):</span><br><span class="line">    <span class="comment"># n is the number of samples</span></span><br><span class="line">    <span class="comment"># pred, y shape: [n, 1]</span></span><br><span class="line">    loss = (-<span class="number">1</span>/n) * (np.<span class="built_in">sum</span>(y * np.log(pred) + (<span class="number">1</span>-y) * np.log(<span class="number">1</span>-pred)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_regression</span>(<span class="params">X, Y</span>):</span><br><span class="line">    <span class="comment"># X: input data</span></span><br><span class="line">    <span class="comment"># Y: labels</span></span><br><span class="line">    n_samples, n_features = X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]</span><br><span class="line">    w, b = weight_init(n_features)</span><br><span class="line">    <span class="comment"># hyper parameters</span></span><br><span class="line">    iteration = <span class="number">10</span></span><br><span class="line">    lr = <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">        <span class="comment"># np.dot(x,y.T) == x @ y.T</span></span><br><span class="line">        pred = sigmoid(np.dot(X, w.T) + b)</span><br><span class="line">        loss = (pred, Y, n_samples)</span><br><span class="line">        <span class="comment"># calculate gradients</span></span><br><span class="line">        <span class="comment"># reference: https://laobadao.github.io/2017/10/27/logistic-cost/index.html</span></span><br><span class="line">        <span class="comment"># https://gist.github.com/golamSaroar/1a1ce33139cf77c37cd6bd123f0fa9cb</span></span><br><span class="line">        w_grad = (<span class="number">1</span>/n_samples) * (np.dot(X.T, (pred - Y.T).T))</span><br><span class="line">        b_grad = (<span class="number">1</span>/n_samples) * （np.<span class="built_in">sum</span>(pred - Y.T)）</span><br><span class="line">        <span class="comment"># update w, b</span></span><br><span class="line">        w -= lr * w_grad</span><br><span class="line">        b -= lr * b_grad</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>

<h3><span id="linear-regression">Linear Regression</span></h3><p>NumPy implementation for multiple linear regression (two or more features for each data point), simple linear regression (only one feature for each data point)<br><a target="_blank" rel="noopener" href="https://www.mvrlink.com/linear-regression-from-scratch-with-numpy/">reference</a>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean-squared error loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_pred, y</span>):</span><br><span class="line">    n = y.shape[<span class="number">0</span>] <span class="comment"># number of samples</span></span><br><span class="line">    loss = (<span class="number">1</span>/n) * np.<span class="built_in">sum</span>((y - y_pred)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainable weights and bias init</span></span><br><span class="line"><span class="comment"># n_features &gt;= 2 for multiple linear regression</span></span><br><span class="line"><span class="comment"># n_features = 1 for simple linear regression</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weight_init</span>(<span class="params">n_features</span>):</span><br><span class="line">    w = np.zeros(<span class="number">1</span>, n_features)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradientDescent</span>(<span class="params">n_iteration, x, y, lr=<span class="number">1e-5</span></span>):</span><br><span class="line">    n_samples, n_features = x.shape[<span class="number">0</span>], x.shape[<span class="number">1</span>]</span><br><span class="line">    w, b = weight_init(n_features)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_iteration):</span><br><span class="line">        y_pred = x @ w.T + b</span><br><span class="line">        mse_loss = (y_pred, y)</span><br><span class="line">        w_grad = (<span class="number">1</span> / n_samples) * np.dot(x.T, y_pred - y)</span><br><span class="line">        b_grad = (<span class="number">1</span> / n_samples) * np.<span class="built_in">sum</span>(y - y_pred)</span><br><span class="line">        w -= lr * w_grad</span><br><span class="line">        b -= lr * b_grad</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>

<h3><span id="activation-functions">Activation Functions</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261744882.png" alt="activation_functions"></p>
<ul>
<li><p>sigmoid (value from 0 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>ReLU (from 0 to infinity)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
</li>
<li><p>Leaky ReLU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, alpha=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the Leaky ReLU activation.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(alpha * x, x)</span><br></pre></td></tr></table></figure>
</li>
<li><p>tanh (from -1 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tahn</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> (np.exp(x)-np.exp(-x)) / (np.exp(x) + np.exp(-x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>Softmax (from 0 to 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># assumes x is a vector</span></span><br><span class="line">    x = x - np.<span class="built_in">max</span>(x) <span class="comment"># to avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.<span class="built_in">sum</span>(np.exp(x))</span><br></pre></td></tr></table></figure>
</li>
<li><p>GLU (Gated Linear Units) activation function<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022009002.png" alt="glu"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GLU</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(in_size, in_size)</span><br><span class="line">        self.linear2 = nn.Linear(in_size, in_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear1(x) * self.linear2(x).sigmoid()</span><br></pre></td></tr></table></figure></li>
<li><p>Sigmoid Linear Unit (SiLU/swish) activation function<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402040009042.png" alt="silu"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">silu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the SiLU activation for numpy arrays.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> x / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>

<h3><span id="loss-functions">Loss Functions</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261757405.jpeg" alt="loss_func"></p>
<ul>
<li><p>Binary Cross Entropy Loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">binary_cross_entropy</span>(<span class="params">actual, predicted</span>):</span><br><span class="line">    sum_score = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(actual)):</span><br><span class="line">        sum_score += actual[i] * np.log(<span class="number">1e-10</span> + predicted[i])</span><br><span class="line">    mean_sum_score = -<span class="number">1.0</span> / <span class="built_in">len</span>(actual) * sum_score</span><br></pre></td></tr></table></figure>
</li>
<li><p>Categorical Cross Entropy Loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">categorical_cross_entropy</span>(<span class="params">actual, predicted</span>):</span><br><span class="line">    sum_score = <span class="number">0.0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(actual)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(actual[i])):</span><br><span class="line">            sum_score += actual[i][j] * np.log(<span class="number">1e-10</span> + predicted[i][j])</span><br><span class="line">    mean_sum_score = -<span class="number">1.0</span> /<span class="built_in">len</span>(actual) * sum_score</span><br><span class="line">    <span class="keyword">return</span> mean_sum_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># class CrossEntropyLoss():</span></span><br><span class="line"><span class="comment">#     def forward(self, in_probs, labels):</span></span><br><span class="line"><span class="comment">#         self.in_probs = self.in_probs</span></span><br><span class="line"><span class="comment">#         self.labels = labels</span></span><br><span class="line"><span class="comment">#         return np.sum(-labels*np.log(self.in_probs))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     def backward(self):</span></span><br><span class="line"><span class="comment">#         return -self.labels/self.in_probs</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3><span id="evaluation-metrics">Evaluation metrics</span></h3><h4><span id="recall-precision-f1-accuracy">Recall, Precision, F1, accuracy</span></h4><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072019388.png" alt="rpf1"></p>
<h4><span id="marco-amp-micro-average-recallprecision">Marco &amp; Micro average recall/precision</span></h4><p>A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In macro averaging, we compute the performance for each class, and then average over classes. In micro averaging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and recall from that table.<br>In a multi-class classification setup, <strong>micro-average is preferable</strong> if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072025852.png" alt="micro_marco"><br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072027841.png" alt="micr0_marco_2"><br>Macro-averaged metrics are used when we want to evaluate systems performance across on different datasets.<br>Micro-averaged metrics should be used when the size of datasets are variable.</p>
<h4><span id="roc-and-auc">ROC and AUC</span></h4><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041721113.png" alt="roc"></p>
<p>A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values. The ROC curve is the plot of the <em>true positive rate (TPR)</em> against the <em>false positive rate (FPR)</em> at each threshold setting.</p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041730706.png" alt="tpr_fpr"></p>
<p>AUC stands for “Area under the ROC Curve.” That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.</p>
<h4><span id="bleu-score">BLEU score</span></h4><p>BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. BLEU’s output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts.</p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041813997.png" alt="bleu"></p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041816809.png" alt="belu_cal"></p>
<h4><span id="rouge-score">Rouge score</span></h4><p>ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.</p>
<ul>
<li>ROUGE-N: Overlap of n-grams between the system and reference summaries.</li>
<li>ROUGE-L: Longest Common Subsequence (LCS) based statistics. Longest common subsequence problem takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822903.png" alt="rouge"></p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041822446.png" alt="rouge_p_r"></p>
<h4><span id="perplexity">Perplexity</span></h4><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041843225.png" alt="Perplexity"></p>
<p>Perplexity (PPL) is one of the most common metrics for evaluating language models. Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. Intuitively, it can be thought of as an evaluation of the model’s ability to predict uniformly among the set of specified tokens in a corpus. The lower perplexity means the model has better performance on the specific corpus.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041856323.png" alt="log_ppl"></p>
<h4><span id="mrr-map-ndcg">MRR, MAP, NDCG</span></h4><ul>
<li><p>MRR<br>Mean Reciprocal Rank (MRR) is one of the metrics that help evaluate the quality of recommendation and information retrieval systems. Mean Reciprocal Rank (MRR) defines how good a retrieval system is to return a relevant result as the top result. To calculate the MRR, multiple queries are invoked. If all of these queries return a relevant document at the top, the score will be 1.0.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041959054.png" alt="mrr"></p>
</li>
<li><p>MAP<br>The disadvantage of MRR is that only the top document is considered. Other use cases like comparative questions in Question Answering scenarios require multiple relevant documents. Mean Average Precision (MAP) addresses this shortcoming by considering the order of the returned items. To calculate this score, three steps are taken:</p>
<ol>
<li>Calculate precision</li>
<li>Calculate average precision for K</li>
<li>Calculate mean average precision for sample of queries<br>As for MRR, multiple queries are run. Instead of calculating the Recall, Precision is computed by using relevant and non-relevant documents from the returned lists (not the complete dataset).<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042007454.png" alt="map"></li>
</ol>
</li>
</ul>
<p>Average Precision (AP) is a widely used metric in information retrieval that quantifies the quality of a retrieval system’s ranked results for a single query. Unlike precision, which considers the entire ranked list of retrieved items, AP focuses on how well relevant items are ranked within that list. It measures the area under the precision-recall curve for a single query.<br>AP is calculated as follows:</p>
<ol>
<li>First, consider a query that retrieves a set of items.</li>
<li>Compute the precision for each position where a relevant item is retrieved in the ranked results list.</li>
<li>Average these precision values, but only for the positions where relevant items were retrieved.</li>
</ol>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402042051887.png" alt="ap"></p>
<ul>
<li>CG, DCG, NDCG<br>Discounted cumulative gain (DCG) is a measure of ranking quality which is often presented in its normalized and more easily interpretable form, known as Normalized DCG (nDCG or NDCG). In information retrieval, NDCG is often used to measure effectiveness of search engine algorithms and related applications. </li>
</ul>
<p><img src="!%5B%5D(https://raw.githubusercontent.com/ichbinhandsome/images/main/202402041950356.png)" alt="ndcg"></p>
<p>nDCG implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ndcg</span>(<span class="params">golden, current, n = -<span class="number">1</span></span>):</span><br><span class="line">    log2_table = np.log2(np.arange(<span class="number">2</span>, <span class="number">102</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dcg_at_n</span>(<span class="params">rel, n</span>):</span><br><span class="line">        rel = np.asfarray(rel)[:n]</span><br><span class="line">        dcg = np.<span class="built_in">sum</span>(np.divide(np.power(<span class="number">2</span>, rel) - <span class="number">1</span>, log2_table[:rel.shape[<span class="number">0</span>]]))</span><br><span class="line">        <span class="keyword">return</span> dcg</span><br><span class="line"></span><br><span class="line">    ndcgs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(current)):</span><br><span class="line">        k = <span class="built_in">len</span>(current[i]) <span class="keyword">if</span> n == -<span class="number">1</span> <span class="keyword">else</span> n</span><br><span class="line">        idcg = dcg_at_n(<span class="built_in">sorted</span>(golden[i], reverse=<span class="literal">True</span>), n=k)</span><br><span class="line">        dcg = dcg_at_n(current[i], n=k)</span><br><span class="line">        tmp_ndcg = <span class="number">0</span> <span class="keyword">if</span> idcg == <span class="number">0</span> <span class="keyword">else</span> dcg / idcg</span><br><span class="line">        ndcgs.append(tmp_ndcg)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.</span> <span class="keyword">if</span> <span class="built_in">len</span>(ndcgs) == <span class="number">0</span> <span class="keyword">else</span> <span class="built_in">sum</span>(ndcgs) / (<span class="built_in">len</span>(ndcgs))</span><br></pre></td></tr></table></figure>


<h2><span id="deep-learning-algorithms">Deep Learning algorithms</span></h2><h3><span id="batch-normalization">Batch Normalization</span></h3><p>Why normalization?<br>The phenomenon by which the distributions of internal nodes (neurons) of a neural network change is referred to as <strong>Internal Covariate Shift</strong>. And we want to avoid it because it makes training the network slower, as the neurons are forced to re-adjust drastically their weights in one direction or another because of drastic changes in the outputs of the previous layers.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311261845730.png" alt="norm"></p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212101170.png" alt="batch_norm"></p>
<ul>
<li>PyTorch implementation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchnorm_forward</span>(<span class="params">x, gamma, beta, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">    N, D = x.shape <span class="comment"># N is the batch size, D is the hidden size of each sample in batch</span></span><br><span class="line">    sample_mean = x.mean(axis=<span class="number">0</span>) <span class="comment"># shape [1, D]</span></span><br><span class="line">    sample_var = x.var(axis=<span class="number">0</span>) <span class="comment"># shape [1, D]</span></span><br><span class="line">    std = np.sqrt(sample_var + eps)</span><br><span class="line">    x_centered = x - sample_mean</span><br><span class="line">    x_norm = x_centered / std</span><br><span class="line">    out = gamma * x_norm + beta</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li>
<li>NumPy implementation<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batchnorm_forward</span>(<span class="params">x, scales,, bias, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">  mean = x.mean(axis=<span class="number">0</span>)  <span class="comment"># Shape = (w, h, c)</span></span><br><span class="line">  var  = x.var(axis=<span class="number">0</span>)</span><br><span class="line">  std = np.sqrt(var+ eps) <span class="comment"># shape = (w, h, c)</span></span><br><span class="line">  <span class="comment"># eps is used to avoid divisions by zero</span></span><br><span class="line">  <span class="comment"># Compute the normalized input</span></span><br><span class="line">  x_norm = (x - mean) / std <span class="comment"># shape (batch, w, h, c)</span></span><br><span class="line">  <span class="comment"># Output = scale * x_norm + bias</span></span><br><span class="line">  <span class="keyword">if</span> scales <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      x_norm *= scales  <span class="comment"># Multiplication for scales</span></span><br><span class="line">  <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      x_norm += bias <span class="comment"># Add bias</span></span><br><span class="line">  <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3><span id="layer-normalization">Layer Normalization</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202311212149406.png" alt="layer_norm"><br>Explanation:</p>
<blockquote>
<p>A well-known explanation of the success of LayerNorm is its re-centering and re-scaling invariance property. The former enables the model to be insensitive to shift noises on both inputs and weights, and the latter keeps the output representations intact when both inputs and weights are randomly scaled.</p>
</blockquote>
<ul>
<li><p>Numpy implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_norm</span>(<span class="params">x, g, b, eps: <span class="built_in">float</span> = <span class="number">1e-5</span></span>):</span><br><span class="line">    mean = np.mean(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    variance = np.var(x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    x = (x - mean) / np.sqrt(variance + eps)  <span class="comment"># normalize x to have mean=0 and var=1 over last axis</span></span><br><span class="line">    <span class="keyword">return</span> g * x + b  <span class="comment"># scale and offset with gamma/beta params</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Pytorch implementation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># feature : (batch_size, seq_len, hidden_dim)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorma, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">farward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># (batch_size, seq_len, 1)</span></span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># (batch_size, seq_len, 1)</span></span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x-mean) / (std+self.eps) + self.b_2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3><span id="dropout">Dropout</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401022024946.png" alt="dropout"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dropout</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self. drop_prob</span>):</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        self.dropout = (np.random.rand(*inputs.shape) &gt; self.dorp_prob)</span><br><span class="line">        <span class="keyword">return</span> np.where(self.dropout, inputs, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, grads</span>):</span><br><span class="line">        <span class="keyword">return</span> np.where(self.dropout, grads, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3><span id="optimizers">Optimizers</span></h3><h4><span id="sgd-stochastic-gradient-descent">SGD (Stochastic Gradient Descent)</span></h4><p>Advantages:</p>
<ol>
<li>Frequent updates of model parameters, converges in less time</li>
<li>Requires less memory as no need to store values of loss functions</li>
<li>May get new minima’s<br>Disadvantages:</li>
<li>High variance in model parameters</li>
<li>May shoot even after achieving global minima</li>
<li>To get the same convergence as gradient descent needs to slowly reduce the value of learning rate</li>
</ol>
<h4><span id="mini-batch-gradient-descent-bgd">Mini-Batch Gradient Descent (BGD)</span></h4><p>Advantages:</p>
<ol>
<li>Frequently updates the model parameters and also has less variance</li>
<li>Requires medium amount of memory<br>All types of Gradient Descent have some challenges:</li>
<li>May get trapped at local minima</li>
<li>Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge</li>
<li>Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate</li>
</ol>
<h4><span id="momentum">Momentum</span></h4><p>Used in conjunction Stochastic Gradient Descent or Mini-Batch Gradient Descent, Momentum takes into account past gradients to smooth out the update.<br>This is seen in variable 𝑣, which is an exponentially weighted average of the gradient on previous steps. This results in minimizing oscillations and faster convergence.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071937092.png" alt="momentum"><br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071932599.png" alt="moment"><br>Compared with (minibatch) stochastic gradient descent the momentum method needs to maintain a set of auxiliary variables, i.e., velocity. It has the same shape as the gradients (and variables of the optimization problem). In the implementation below we call these variables states.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_momentum_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_momentum</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = hyperparams[<span class="string">&#x27;momentum&#x27;</span>] * v + p.grad</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="adagrad-adaptative-gradient">Adagrad (Adaptative Gradient)</span></h4><p>Adagard optimizer changes the learning rate for each parameter at every time step. Particularly, it tends to assign higher learning rates to infrequent features, which ensures that the parameter updates rely less on frequency and more on relevance.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071928626.png" alt="adagrad"><br>Advantages:</p>
<ol>
<li>Learning rate changes for each training parameters</li>
<li>Don’t need to manually tune the learning rate</li>
<li>Able to train on sparse data<br>Disadvantages:</li>
<li>Computationally expensive as a need to calculate the second order derivative</li>
<li>The learning rate is always decreasing results in slow training</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adagrad_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adagrad</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    eps = <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            s[:] += torch.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / torch.sqrt(s + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="adadelta">Adadelta</span></h4><p>Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071955958.png" alt="adadelta"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adadelta_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w, s_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    delta_w, delta_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((s_w, delta_w), (s_b, delta_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adadelta</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    rho, eps = hyperparams[<span class="string">&#x27;rho&#x27;</span>], <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> p, (s, delta) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># In-place updates via [:]</span></span><br><span class="line">            s[:] = rho * s + (<span class="number">1</span> - rho) * torch.square(p.grad)</span><br><span class="line">            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad</span><br><span class="line">            p[:] -= g</span><br><span class="line">            delta[:] = rho * delta + (<span class="number">1</span> - rho) * g * g</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="rmsprop-root-mean-square-propagation">RMSProp (Root Mean Square Propagation)</span></h4><p>RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale coefficients. RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique to adjust the coefficient-wise preconditioner. The learning rate needs to be scheduled by the experimenter in practice.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401071939471.png" alt="RMSProp"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rmsprop_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    s_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    s_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (s_w, s_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rmsprop</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    gamma, eps = hyperparams[<span class="string">&#x27;gamma&#x27;</span>], <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, s <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            s[:] = gamma * s + (<span class="number">1</span> - gamma) * torch.square(p.grad)</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * p.grad / torch.sqrt(s + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>

<h4><span id="adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)</span></h4><p>Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. Adam computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.<br><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401072005801.png" alt="adam"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w, v_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    s_w, s_b = torch.zeros((feature_dim, <span class="number">1</span>)), torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad</span><br><span class="line">            s[:] = beta2 * s + (<span class="number">1</span> - beta2) * torch.square(p.grad)</span><br><span class="line">            v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr)</span><br><span class="line">                                                       + eps)</span><br><span class="line">        p.grad.data.zero_()</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>Note: <strong>AdamW</strong>(Adam with decoupled weight decay)</p>
<h2><span id="nlp-algorithms">NLP algorithms</span></h2><h3><span id="fasttext">FastText</span></h3><p>fastText is usually used to do classification tasks, it has two main improvements:</p>
<ul>
<li>used char-level n-gram as additional word embedding features</li>
<li>used hierarchical softmax to reduce computation cost<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FastText</span>(nn.module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>(FastText, self).__init__()</span><br><span class="line">        self.embedding_word = nn.Embedding(</span><br><span class="line">            config.vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.vocab_size-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.embedding_bigram = nn.Embedding(</span><br><span class="line">            config.ngram_vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.ngram_vocab_size-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.embedding_trigram = nn.Embedding(</span><br><span class="line">            config.ngram_vocab_size, </span><br><span class="line">            config.embed_size,</span><br><span class="line">            padding_idx = config.ngram_vocab_size-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout_rate)</span><br><span class="line">        self.fc1 = nn.Linear(config.embed_size * <span class="number">3</span>, config.hidden_size)</span><br><span class="line">        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param x: x[0] (words), x[1] (bigram), x[2] (trigram): all have shape [batch_size, seq_length] </span></span><br><span class="line"><span class="string">        :return: [batch_size, num_classes] classification task</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        embed_bow = self.embedding_word(x[<span class="number">0</span>])</span><br><span class="line">        embed_bigram = self.embedding_bigram(x[<span class="number">1</span>])</span><br><span class="line">        embed_trigram = self.embedding_trigram(x[<span class="number">2</span>])</span><br><span class="line">        <span class="built_in">input</span> = torch.cat((embed_bow, embed_bigram, embed_trigram), -<span class="number">1</span>)</span><br><span class="line">        out = <span class="built_in">input</span>.mean(dim=<span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2><span id="llm-algorithms">LLM algorithms</span></h2><h3><span id="lora-low-rank-adaption-implementation">LoRA (Low Rank Adaption) Implementation</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031604097.png" alt="lora"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearLoRA</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    A low rank adapted linear layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_dim: An integer representing the input dimension of the linear layers</span></span><br><span class="line"><span class="string">        out_dim: An integer representing the output dimension of the linear layer</span></span><br><span class="line"><span class="string">        r: An integer representing the rank of low-rank approximated matrices</span></span><br><span class="line"><span class="string">        lora_alpha: An integer representing the number of scaling constant alpha / r</span></span><br><span class="line"><span class="string">        lora_dropout: A float between 0 and 1 representing the dropout probability</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim:<span class="built_in">int</span>, out_dim:<span class="built_in">int</span>, r: <span class="built_in">int</span>=<span class="number">8</span>, lora_alpha: <span class="built_in">int</span>=<span class="number">16</span>, lora_dropout: <span class="built_in">float</span>=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.r = r <span class="comment"># must &gt;= 1</span></span><br><span class="line">        self.lora_alpha = lora_alpha</span><br><span class="line">        self.lora_dropout = nn.Dropout(lora_dropout)</span><br><span class="line">        <span class="comment"># recreate the linear layer and freeze it (the actual weight values will be copied in outside of this class)</span></span><br><span class="line">        self.pretrained = nn.Linear(in_dim, out_dim, bias=<span class="literal">True</span>)</span><br><span class="line">        self.pretrained.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># create the low-rank A matrix and initialize with same method as in Hugging face PEFT</span></span><br><span class="line">        self.lora_A = nn.Linear(in_dim, r, bias=<span class="literal">True</span>)</span><br><span class="line">        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(<span class="number">5</span>)) <span class="comment"># or nn.init.normal_(self.lora_A, mean=0, std=1)</span></span><br><span class="line">        <span class="comment"># another method to define LoRA_A matrix</span></span><br><span class="line">        <span class="comment"># self.A = nn.Parameter(torch.randn(in_dim, r) * math.sqrt(r))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># create the low rank B matrix and initialize to zero</span></span><br><span class="line">        self.lora_B = nn.Linear(r, out_dim, bias=<span class="literal">True</span>)</span><br><span class="line">        nn.init.constant_(self.lora_B.weight, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># another method to define LoRA_B matrix</span></span><br><span class="line">        <span class="comment"># self.B = nn.Parameter(torch.zeros(r, out_dim))</span></span><br><span class="line">        self.scaling = self.lora_alpha / self.r</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        pretrained_out = self.pretrained(x)</span><br><span class="line">        lora_out = self.lora_A(x)</span><br><span class="line">        lora_out = self.lora_B(lora_out)</span><br><span class="line">        <span class="comment"># if we used another method defined A and B:</span></span><br><span class="line">        <span class="comment"># lora_out = x @ self.A @ self.B</span></span><br><span class="line">        lora_out = self.lora_dropout(lora_out)</span><br><span class="line">        lora_out = lora_out * self.scaling</span><br><span class="line">        <span class="keyword">return</span> pretrained_out + lora_out</span><br><span class="line"></span><br><span class="line"><span class="comment">## Another simpale to implement LoRA</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoRALayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, out_dim, rank, alpha</span>):</span><br><span class="line">        std_dev = <span class="number">1</span> / torch.sqrt(torch.tensort(rank).<span class="built_in">float</span>())</span><br><span class="line">        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)</span><br><span class="line">        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.alpha * (x @ self.A @ self.B)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h3><span id="speculative-decoding">Speculative Decoding</span></h3><p>This technology is used in LLM inference stage to speed up Auto-regressive decoding steps.<br>Main idea: (<a target="_blank" rel="noopener" href="https://jaykmody.com/blog/speculative-sampling/">source</a>)</p>
<blockquote>
<p>In speculative sampling, we have two models:</p>
<ol>
<li>A smaller, faster draft model</li>
<li>A larger, slower target model<br>The idea is that the draft model speculates what the output is steps into the future, while the target model determines how many of those tokens we should accept.</li>
</ol>
</blockquote>
<p>The intuition behind speculative sampling is that <em>certain strings of tokens (common phrases, pronouns, punctuation, etc …) are fairly easy to predict, so a smaller, less powerful, but faster draft model should be able to quickly predict these instead of having our slower target model doing all the work</em>.</p>
<p>The algorithm is following:</p>
<ol>
<li>The draft model decodes <em>K</em> tokens in the regular autoregressive fashion.</li>
<li>We get the probability outputs of the target and draft model on the new predicted sequence.</li>
<li>We compare the target and draft model probabilities to determine how many of the <em>K</em> tokens we want to keep based on some rejection criteria. If a token is rejected, we resample it using a combination of the two distributions and don’t accept any more tokens.</li>
<li>If all <em>K</em> tokens are accepted, we can sample an additional final token from the target model probability output.</li>
</ol>
<p>As such, instead of decoding a single token at each iteration, speculative sampling decodes between 1 to <em>K+1</em> tokens per iteration. If no tokens are accepted, we resample guaranteeing at least 1 token is decoded. If all <em>K</em> tokens are accepted, then we can also sample a final token from the target models probability distribution, giving us a total of <em>K+1</em> tokens decoded.</p>
<p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202402031620531.png" alt="sepcu_decoding"></p>
<h2><span id="mathematics">Mathematics</span></h2><h3><span id="different-distributions">Different distributions</span></h3><p><img src="https://raw.githubusercontent.com/ichbinhandsome/images/main/202401312314722.png" alt="dist"></p>
<h2><span id="references">References</span></h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wevolf/p/15195143.html">https://www.cnblogs.com/wevolf/p/15195143.html</a></li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">https://lilianweng.github.io/posts/2021-05-31-contrastive/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hkproj/pytorch-llama/tree/main">https://github.com/hkproj/pytorch-llama/tree/main</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.07467.pdf">https://arxiv.org/pdf/1910.07467.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6">https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6</a></li>
<li><a target="_blank" rel="noopener" href="https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum">https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.15596.pdf">https://arxiv.org/pdf/2211.15596.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://spotintelligence.com/2023/09/14/mean-average-precision/">https://spotintelligence.com/2023/09/14/mean-average-precision/</a></li>
</ul>

    </div>

    
    
    
        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/ichbinhandsome">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">GitHub</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/besser-46">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/17/job-summary/" rel="prev" title="2021秋招回顾">
      <i class="fa fa-chevron-left"></i> 2021秋招回顾
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/10/22/Attention-Implementaion/" rel="next" title="Multi-Head Self Attention Implementation">
      Multi-Head Self Attention Implementation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">Classic Machine Learning algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">K-means</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text">Logistic regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.</span> <span class="nav-text">Linear Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.4.</span> <span class="nav-text">Activation Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.5.</span> <span class="nav-text">Loss Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.6.</span> <span class="nav-text">Evaluation metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.1.</span> <span class="nav-text">Recall, Precision, F1, accuracy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.2.</span> <span class="nav-text">Marco &amp; Micro average recall&#x2F;precision</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.3.</span> <span class="nav-text">ROC and AUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.4.</span> <span class="nav-text">BLEU score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.5.</span> <span class="nav-text">Rouge score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.6.</span> <span class="nav-text">Perplexity</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.7.</span> <span class="nav-text">MRR, MAP, NDCG</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">Deep Learning algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">Batch Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.2.</span> <span class="nav-text">Layer Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.3.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.4.</span> <span class="nav-text">Optimizers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.1.</span> <span class="nav-text">SGD (Stochastic Gradient Descent)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.2.</span> <span class="nav-text">Mini-Batch Gradient Descent (BGD)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.3.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.4.</span> <span class="nav-text">Adagrad (Adaptative Gradient)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.5.</span> <span class="nav-text">Adadelta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.6.</span> <span class="nav-text">RMSProp (Root Mean Square Propagation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">2.4.7.</span> <span class="nav-text">Adam (Adaptive Moment Estimation)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">NLP algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.</span> <span class="nav-text">FastText</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">LLM algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.1.</span> <span class="nav-text">LoRA (Low Rank Adaption) Implementation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.2.</span> <span class="nav-text">Speculative Decoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">Mathematics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.1.</span> <span class="nav-text">Different distributions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">6.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruixiang Wang</p>
  <div class="site-description" itemprop="description">somthing interesting</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ichbinhandsome" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ichbinhandsome" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/besser-46" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;besser-46" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/ruixiang-wang-85769a199" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;ruixiang-wang-85769a199" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.kaggle.com/wangruixiang07" title="Kaggle → https:&#x2F;&#x2F;www.kaggle.com&#x2F;wangruixiang07" rel="noopener" target="_blank"><i class="fab fa-kaggle fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fas fa-biohazard"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruixiang Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"bgBMKRcFSCM47e8KkwhkNhxA-MdYXbMMI","app_key":"gyzjWP17vkNtWyHEjWdY5c8Y","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":250,"height":500},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
